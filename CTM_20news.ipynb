{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/group/cup/data1/akash_itm/IPC_MAP/lib/python2.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools,time\n",
    "import sys, os\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import pickle\n",
    "import tensorflow.contrib.distributions.python.ops as ops\n",
    "from tensorflow.contrib.distributions.python.ops import operator_pd_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_tr = '/afs/inf.ed.ac.uk/group/cup/data1/akash_itm\\\n",
    "/IPC_MAP/NTP/tensorflow_lda/lda-tf_new/data/20news_clean/train.txt.npy'\n",
    "data_tr = np.load(dataset_tr)\n",
    "dataset_te = '/afs/inf.ed.ac.uk/group/cup/data1/akash_itm\\\n",
    "/IPC_MAP/NTP/tensorflow_lda/lda-tf_new/data/20news_clean/test.txt.npy'\n",
    "data_te = np.load(dataset_te)\n",
    "vocab = '/afs/inf.ed.ac.uk/group/cup/data1/akash_itm\\\n",
    "/IPC_MAP/NTP/tensorflow_lda/lda-tf_new/data/20news_clean/vocab.pkl'\n",
    "vocab = pickle.load(open(vocab,'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11258, 1995)\n",
      "(7487, 1995)\n"
     ]
    }
   ],
   "source": [
    "vocab_size=len(vocab)\n",
    "def onehot(data, min_length):\n",
    "    return np.bincount(data, minlength=min_length)\n",
    "data_tr = np.array([onehot(doc.astype('int'),vocab_size) for doc in data_tr if np.sum(doc)!=0])\n",
    "data_te = np.array([onehot(doc.astype('int'),vocab_size) for doc in data_te if np.sum(doc)!=0])\n",
    "print data_tr.shape\n",
    "print data_te.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples_tr = data_tr.shape[0]\n",
    "n_samples_te = data_te.shape[0]\n",
    "docs_tr = data_tr\n",
    "docs_te = data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def xavier_init(fan_in, fan_out, constant=1): \n",
    "    \"\"\" Xavier initialization of network weights\"\"\"\n",
    "    # https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "    low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "    high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out), \n",
    "                             minval=low, maxval=high, \n",
    "                             dtype=tf.float32)\n",
    "def log_dir_init(fan_in, fan_out,topics=50): \n",
    "    return tf.log((1.0/topics)*tf.ones([fan_in, fan_out]))\n",
    "\n",
    "tf.reset_default_graph()\n",
    "class VariationalAutoencoder(object):\n",
    "    \"\"\" \n",
    "    See \"Auto-Encoding Variational Bayes\" by Kingma and Welling for more details.\n",
    "    \"\"\"\n",
    "    def __init__(self, network_architecture, transfer_fct=tf.nn.softplus, \n",
    "                 learning_rate=0.001, batch_size=100):\n",
    "        self.network_architecture = network_architecture\n",
    "        self.transfer_fct = transfer_fct\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        print 'Learning Rate:', self.learning_rate\n",
    "        \n",
    "        # tf Graph input\n",
    "        self.x = tf.placeholder(tf.float32, [None, network_architecture[\"n_input\"]])\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        self.is_training = tf.placeholder(tf.bool, [], name='is_training')\n",
    "        \n",
    "        self.h_dim = float(network_architecture[\"n_z\"])\n",
    "        self.nz=network_architecture[\"n_z\"]\n",
    "        \n",
    "        self.a = np.ones((self.batch_size , self.h_dim)).astype(np.float32)/self.h_dim\n",
    "        \n",
    "#         self.mu2 = tf.constant(0.0)\n",
    "#         self.var2 = tf.constant(  np.ones(self.batch_size,self.h_dim).astype(np.float32)  )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Create autoencoder network\n",
    "        self._create_network()\n",
    "        # Define loss function based variational upper-bound and \n",
    "        # corresponding optimizer\n",
    "        self._create_loss_optimizer()\n",
    "        \n",
    "        # Initializing the tensor flow variables\n",
    "        init = tf.initialize_all_variables()\n",
    "\n",
    "        # Launch the session\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(init)\n",
    "    \n",
    "    def _create_network(self):\n",
    "        # Initialize autoencode network weights and biases\n",
    "        self.network_weights = self._initialize_weights(**self.network_architecture)\n",
    "        '''get mean and sigma'''\n",
    "            \n",
    "        self.z_mean,self.log_chol = \\\n",
    "            self.mean_network(self.network_weights[\"sub_01\"], \n",
    "                                      self.network_weights[\"biases_sub_01\"])\n",
    "        \n",
    "#         self.log_chol = \\\n",
    "#             self.sigma_network(self.network_weights[\"sub_02\"], \n",
    "#                                       self.network_weights[\"biases_sub_02\"])\n",
    "\n",
    "        # Draw one sample z from Gaussian distribution\n",
    "\n",
    "        eps = tf.random_normal((1, self.nz), 0, 1, dtype=tf.float32)\n",
    "        '''Adding RT for subtopics'''\n",
    "        \n",
    "        self.chol = tf.exp(self.log_chol)\n",
    "        \n",
    "        self.sigma = tf.batch_matmul(self.chol,self.chol,adj_y=True)\n",
    "        \n",
    "#         self.sigma_log_det = 2*tf.reduce_sum(tf.log(tf.diag(tf.cholesky(self.sigma))),1)\n",
    "        \n",
    "        temp_calc=tf.squeeze(tf.map_fn(lambda i:tf.matmul(eps,tf.gather(tf.sqrt(self.sigma) ,i) ),\n",
    "                                                   tf.constant(np.arange(self.batch_size)), dtype=tf.float32))\n",
    "        \n",
    "        self.z = tf.add(self.z_mean, temp_calc)\n",
    "\n",
    "        \n",
    "        '''Adding reconstruction'''\n",
    "        \n",
    "        self.x_reconstr_mean = \\\n",
    "            self._generator_network(self.z,self.network_weights[\"weights_gener\"],\n",
    "                                    self.network_weights[\"biases_gener\"])\n",
    "            \n",
    "        \n",
    "            \n",
    "    def _initialize_weights(self, n_hidden_recog_1, n_hidden_recog_2, \n",
    "                            n_hidden_gener_1,  \n",
    "                            n_input, n_z):\n",
    "        all_weights = dict()\n",
    "\n",
    "        with tf.variable_scope(\"sub01\"):\n",
    "            all_weights['sub_01'] = {\n",
    "                'h1': tf.get_variable('h1',[n_input, n_hidden_recog_1]),\n",
    "                'h2': tf.get_variable('h2',[n_hidden_recog_1, n_hidden_recog_2]),\n",
    "                'out_mean': tf.get_variable('out_mean',[n_hidden_recog_2, n_z]),\n",
    "                'out_log_sigma': tf.get_variable('out_log_sigma',[n_hidden_recog_2, n_z*n_z])}\n",
    "            all_weights['biases_sub_01'] = {\n",
    "                'b1': tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32)),\n",
    "                'b2': tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32)),\n",
    "                'out_mean': tf.Variable(tf.zeros([n_z], dtype=tf.float32)),\n",
    "                'out_log_sigma': tf.Variable(tf.zeros([n_z*n_z], dtype=tf.float32))\n",
    "            }\n",
    "        with tf.variable_scope(\"sub02\"):\n",
    "            all_weights['sub_02'] = {\n",
    "                'h1': tf.get_variable('h1',[n_input, n_hidden_recog_1]),\n",
    "                'h2': tf.get_variable('h2',[n_hidden_recog_1, n_hidden_recog_2]),\n",
    "                'out_log_sigma': tf.get_variable('out_log_sigma',[n_hidden_recog_2, n_z*n_z])\n",
    "            }\n",
    "            all_weights['biases_sub_02'] = {\n",
    "                'b1': tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32)),\n",
    "                'b2': tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32)),\n",
    "                'out_log_sigma': tf.Variable(tf.zeros([n_z*n_z], dtype=tf.float32))\n",
    "            }\n",
    "        with tf.variable_scope(\"gen\"):\n",
    "            all_weights['weights_gener'] = {\n",
    "                'h2': tf.Variable(xavier_init(n_z, n_hidden_gener_1))\n",
    "            }\n",
    "            all_weights['biases_gener'] = {\n",
    "                'b2': tf.Variable(tf.zeros([n_hidden_gener_1], dtype=tf.float32))\n",
    "            }\n",
    "        return all_weights\n",
    "            \n",
    "    def mean_network(self, weights, biases):\n",
    "        # Generate probabilistic encoder (recognition network)\n",
    "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.x, weights['h1']), \n",
    "                                           biases['b1'])) \n",
    "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
    "                                           biases['b2'])) \n",
    "        layer_do = tf.nn.dropout(layer_2, self.keep_prob)\n",
    "        \n",
    "        z_mean = tf.contrib.layers.batch_norm(tf.add(tf.matmul(layer_do, weights['out_mean']),\n",
    "                        biases['out_mean']),is_training=self.is_training,updates_collections=None)    \n",
    "        \n",
    "        z_log_sigma_sq = \\\n",
    "            tf.contrib.layers.batch_norm(tf.add(tf.matmul(layer_do, weights['out_log_sigma']), \n",
    "                   biases['out_log_sigma']),is_training=self.is_training,updates_collections=None)\n",
    "        \n",
    "        return z_mean,tf.reshape(z_log_sigma_sq, [self.batch_size,self.nz, self.nz])\n",
    "    \n",
    "    def sigma_network(self, weights, biases):\n",
    "        # Generate probabilistic encoder (recognition network)\n",
    "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.x, weights['h1']), \n",
    "                                           biases['b1'])) \n",
    "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
    "                                           biases['b2'])) \n",
    "        layer_do = tf.nn.dropout(layer_2, self.keep_prob)\n",
    "        \n",
    "#         z_sigma_sq = \\\n",
    "#             tf.exp(tf.contrib.layers.batch_norm(tf.add(tf.matmul(layer_do, weights['out_log_sigma']), \n",
    "#                    biases['out_log_sigma'])))  \n",
    "        z_log_sigma_sq = \\\n",
    "            tf.contrib.layers.batch_norm(tf.add(tf.matmul(layer_do, weights['out_log_sigma']), \n",
    "                   biases['out_log_sigma']),is_training=self.is_training,updates_collections=None) \n",
    "        \n",
    "        return tf.reshape(z_log_sigma_sq, [self.batch_size,self.nz, self.nz])\n",
    "\n",
    "    def _generator_network(self,z, weights, biases):\n",
    "        # Generate probabilistic decoder (decoder network)\n",
    "        self.layer_do_0 = tf.nn.dropout(tf.nn.softmax(tf.contrib.layers.batch_norm(z)), self.keep_prob)\n",
    "        x_reconstr_mean = tf.add(tf.matmul(self.layer_do_0, \n",
    "                                           tf.nn.softmax(tf.contrib.layers.batch_norm(weights['h2']))),0.0)\n",
    "#         x_reconstr_mean = tf.nn.softmax(tf.contrib.layers.batch_norm(tf.add(\n",
    "#                     tf.matmul(self.layer_do_0, weights['h2']),0.0),is_training=self.is_training,updates_collections=None))\n",
    "    \n",
    "        return x_reconstr_mean\n",
    "\n",
    "            \n",
    "    def _create_loss_optimizer(self):\n",
    "        \n",
    "        self.x_reconstr_mean+=1e-10\n",
    "     \n",
    "        reconstr_loss = \\\n",
    "            -tf.reduce_sum(self.x * tf.log(self.x_reconstr_mean),1)#/tf.reduce_sum(self.x,1) \n",
    "            \n",
    "#         temp_s=self.sigma.add_to_tensor(0.0)\n",
    "\n",
    "        temp_trace=tf.squeeze(tf.map_fn(lambda i:tf.trace(tf.gather(self.sigma,i)),\n",
    "                                                   tf.constant(np.arange(self.batch_size)), dtype=tf.float32))\n",
    "        \n",
    "        temp_log_det=tf.squeeze(tf.map_fn(lambda i:2*tf.reduce_sum(tf.log(tf.diag_part(tf.gather(self.chol,i)))),\n",
    "                                                   tf.constant(np.arange(self.batch_size)), dtype=tf.float32))\n",
    "        print temp_trace\n",
    "        print temp_log_det\n",
    "        \n",
    "#         latent_loss = 0.5*( temp_trace+\\\n",
    "#         tf.reduce_sum( tf.mul(tf.div((self.mu2 - self.z_mean),self.var2),\n",
    "#                   (self.mu2 - self.z_mean)),1) - self.h_dim +\\\n",
    "#                            tf.reduce_sum(tf.log(self.var2),1)  - tf.log(temp_det+1e-10))\n",
    "\n",
    "        \n",
    "        latent_loss = 0.5*( temp_trace +\\\n",
    "        tf.reduce_sum(tf.square(self.z_mean),1) - self.h_dim +\\\n",
    "                           tf.log(1.0) - temp_log_det )\n",
    "\n",
    "\n",
    "        self.cost = tf.reduce_mean(reconstr_loss) \\\n",
    "    + tf.reduce_mean(latent_loss) # average over batch\n",
    "        \n",
    "        # Use ADAM optimizer\n",
    "        self.optimizer = \\\n",
    "            tf.train.AdamOptimizer(learning_rate=self.learning_rate,beta1=0.9).minimize(self.cost)\n",
    "        \n",
    "    def partial_fit(self, X):\n",
    "        \"\"\"Train model based on mini-batch of input data.\n",
    "        \n",
    "        Return cost of mini-batch.\n",
    "        \"\"\"\n",
    "#         opt, cost,emb,kld = self.sess.run((self.optimizer, self.cost,self.network_weights['weights_gener']['h2'],\n",
    "#                                           self.kld),feed_dict={self.x: X,self.keep_prob: .9})\n",
    "        opt,cost,emb = self.sess.run((self.optimizer, self.cost,self.network_weights['weights_gener']['h2']\n",
    "                                      ),feed_dict={self.x: X,self.keep_prob: .8,self.is_training:True})\n",
    "        return cost,emb\n",
    "    \n",
    "    def test(self, X):\n",
    "        \"\"\"Test the model and return the lowerbound on the log-likelihood.\n",
    "        \"\"\"\n",
    "        cost = self.sess.run((self.cost),\n",
    "                                        feed_dict={self.x: X\n",
    "                                                   ,self.keep_prob: 1.0,self.is_training:False})\n",
    "        return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "cost_plot=[]\n",
    "def train(network_architecture, learning_rate=0.0001,\n",
    "          batch_size=100, training_epochs=10, display_step=5):\n",
    "    vae = VariationalAutoencoder(network_architecture, \n",
    "                                 learning_rate=learning_rate, \n",
    "                                 batch_size=batch_size)\n",
    "    emb=0\n",
    "    avg_kld=0\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(n_samples_tr / batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs = minibatches.next()\n",
    "            # Fit training using batch data\n",
    "            cost,emb = vae.partial_fit(batch_xs)\n",
    "#             print cost\n",
    "#             sys.exit()\n",
    "            # Compute average loss\n",
    "            avg_cost += cost / n_samples_tr * batch_size\n",
    "            \n",
    "            if np.isnan(avg_cost):\n",
    "                print epoch,i,np.sum(batch_xs,1).astype(np.int),batch_xs.shape\n",
    "                return vae,emb\n",
    "                sys.exit()\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            cost_plot.append(avg_cost)\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \\\n",
    "                  \"cost=\", \"{:.9f}\".format(avg_cost)\n",
    "\n",
    "    return vae,emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.002\n",
      "Tensor(\"Squeeze_1:0\", shape=(200,), dtype=float32)\n",
      "Tensor(\"Squeeze_2:0\", shape=(200,), dtype=float32)\n",
      "Epoch: 0001 cost= 21558446746.518524170\n",
      "Epoch: 0006 cost= 18799973.450121231\n",
      "Epoch: 0011 cost= 3207284.269450058\n",
      "Epoch: 0016 cost= 2069350.431958454\n",
      "Epoch: 0021 cost= 993731.808066694\n",
      "Epoch: 0026 cost= 1927285.826745426\n",
      "Epoch: 0031 cost= 1368996.354059682\n",
      "Epoch: 0036 cost= 520350.424038739\n",
      "Epoch: 0041 cost= 272423.991394314\n",
      "Epoch: 0046 cost= 180626.413239821\n",
      "Epoch: 0051 cost= 832572.747938271\n",
      "Epoch: 0056 cost= 71604.890038264\n",
      "Epoch: 0061 cost= 188824.411746439\n",
      "Epoch: 0066 cost= 153476.202070542\n",
      "Epoch: 0071 cost= 86457.187102713\n",
      "Epoch: 0076 cost= 76620.693427035\n",
      "Epoch: 0081 cost= 57580.622078466\n",
      "Epoch: 0086 cost= 50271.285219747\n",
      "Epoch: 0091 cost= 95750.663824822\n",
      "Epoch: 0096 cost= 125940.983512419\n",
      "Epoch: 0101 cost= 17943.740570941\n",
      "Epoch: 0106 cost= 33126.058717620\n",
      "Epoch: 0111 cost= 47842.151145644\n",
      "Epoch: 0116 cost= 17959.998870595\n",
      "Epoch: 0121 cost= 13280.369324175\n",
      "Epoch: 0126 cost= 19047.757072401\n",
      "Epoch: 0131 cost= 63298.605713715\n",
      "Epoch: 0136 cost= 16081.724011883\n",
      "Epoch: 0141 cost= 17507.539204760\n",
      "Epoch: 0146 cost= 10164.227174565\n",
      "Epoch: 0151 cost= 13383.065740057\n",
      "Epoch: 0156 cost= 23195.566711588\n",
      "Epoch: 0161 cost= 47007.554280845\n",
      "Epoch: 0166 cost= 11882.202994190\n",
      "Epoch: 0171 cost= 9774.827331988\n",
      "Epoch: 0176 cost= 6452.726043251\n",
      "Epoch: 0181 cost= 8151.059797581\n",
      "Epoch: 0186 cost= 7250.521595234\n",
      "Epoch: 0191 cost= 9335.285546077\n",
      "Epoch: 0196 cost= 8122.674587654\n",
      "Epoch: 0201 cost= 6228.525843428\n",
      "Epoch: 0206 cost= 5972.969829094\n",
      "Epoch: 0211 cost= 6626.252516440\n",
      "Epoch: 0216 cost= 5307.839008927\n",
      "Epoch: 0221 cost= 5755.136976379\n",
      "Epoch: 0226 cost= 5566.751742164\n",
      "Epoch: 0231 cost= 4454.848461927\n",
      "Epoch: 0236 cost= 4868.871653942\n",
      "Epoch: 0241 cost= 4268.303550982\n",
      "Epoch: 0246 cost= 5870.971701373\n",
      "Epoch: 0251 cost= 4825.864194489\n",
      "Epoch: 0256 cost= 4010.959905942\n",
      "Epoch: 0261 cost= 3892.368347960\n",
      "Epoch: 0266 cost= 3883.427713556\n",
      "Epoch: 0271 cost= 2997237.626047518\n",
      "Epoch: 0276 cost= 85539.827943098\n",
      "Epoch: 0281 cost= 4096.422841066\n",
      "Epoch: 0286 cost= 4563.829920164\n",
      "Epoch: 0291 cost= 554811.813858149\n",
      "Epoch: 0296 cost= 15081.657266812\n",
      "Epoch: 0301 cost= 7653.311831118\n",
      "Epoch: 0306 cost= 1912.248911191\n",
      "Epoch: 0311 cost= 1978.705513839\n",
      "Epoch: 0316 cost= 1928.974163945\n",
      "Epoch: 0321 cost= 2058.395477365\n",
      "Epoch: 0326 cost= 1837.400444059\n",
      "Epoch: 0331 cost= 1893.169308336\n",
      "Epoch: 0336 cost= 1797.535812641\n",
      "Epoch: 0341 cost= 1798.196618776\n",
      "Epoch: 0346 cost= 1806.947258078\n",
      "Epoch: 0351 cost= 1803.262790471\n",
      "Epoch: 0356 cost= 1777.907470226\n",
      "Epoch: 0361 cost= 1799.424393695\n",
      "Epoch: 0366 cost= 1789.016822846\n",
      "Epoch: 0371 cost= 1743.305578620\n",
      "Epoch: 0376 cost= 1744.217675243\n",
      "Epoch: 0381 cost= 1834.704986697\n",
      "Epoch: 0386 cost= 1723.927138792\n",
      "Epoch: 0391 cost= 1698.453098717\n",
      "Epoch: 0396 cost= 1677.671709059\n",
      "Epoch: 0401 cost= 1681.741245722\n",
      "Epoch: 0406 cost= 28711.455989803\n",
      "Epoch: 0411 cost= 1654.405905540\n",
      "Epoch: 0416 cost= 1612.733035241\n",
      "Epoch: 0421 cost= 1615.890494797\n",
      "Epoch: 0426 cost= 1631.963576769\n",
      "Epoch: 0431 cost= 1570.403685157\n",
      "Epoch: 0436 cost= 1558.466167114\n",
      "Epoch: 0441 cost= 1547.685081494\n",
      "Epoch: 0446 cost= 1519.088108092\n",
      "Epoch: 0451 cost= 1496.123978070\n",
      "Epoch: 0456 cost= 1493.247844068\n",
      "Epoch: 0461 cost= 1460.039859242\n",
      "Epoch: 0466 cost= 1431.629244204\n",
      "Epoch: 0471 cost= 1440.826367222\n",
      "Epoch: 0476 cost= 1544.949507260\n",
      "Epoch: 0481 cost= 1400.181604800\n",
      "Epoch: 0486 cost= 1341.315119058\n",
      "Epoch: 0491 cost= 1323.519188872\n",
      "Epoch: 0496 cost= 1300.069152214\n",
      "Epoch: 0501 cost= 1296.848278551\n",
      "Epoch: 0506 cost= 1251.362368878\n",
      "Epoch: 0511 cost= 1228.022852586\n",
      "Epoch: 0516 cost= 1215.539205974\n",
      "Epoch: 0521 cost= 1193.898312502\n",
      "Epoch: 0526 cost= 1153.487942862\n",
      "Epoch: 0531 cost= 1157.782376022\n",
      "Epoch: 0536 cost= 1088.016920823\n",
      "Epoch: 0541 cost= 1080.633903825\n",
      "Epoch: 0546 cost= 1045.086444388\n",
      "Epoch: 0551 cost= 1043.121698962\n",
      "Epoch: 0556 cost= 1010.488795641\n",
      "Epoch: 0561 cost= 978.066465236\n",
      "Epoch: 0566 cost= 954.992294108\n",
      "Epoch: 0571 cost= 932.271640037\n",
      "Epoch: 0576 cost= 922.032495425\n",
      "Epoch: 0581 cost= 930.565330626\n",
      "Epoch: 0586 cost= 881.195568221\n",
      "Epoch: 0591 cost= 852.031722537\n",
      "Epoch: 0596 cost= 849.248135571\n",
      "Epoch: 0601 cost= 816.052943298\n",
      "Epoch: 0606 cost= 800.312683463\n",
      "Epoch: 0611 cost= 805.207158677\n",
      "Epoch: 0616 cost= 775.883828965\n",
      "Epoch: 0621 cost= 774.759193593\n",
      "Epoch: 0626 cost= 780.144887420\n",
      "Epoch: 0631 cost= 839.312350540\n",
      "Epoch: 0636 cost= 752.467408810\n",
      "Epoch: 0641 cost= 736.829019893\n",
      "Epoch: 0646 cost= 733.034147939\n",
      "Epoch: 0651 cost= 721.478693276\n",
      "Epoch: 0656 cost= 721.547941993\n",
      "Epoch: 0661 cost= 728.111575302\n",
      "Epoch: 0666 cost= 706.294752247\n",
      "Epoch: 0671 cost= 796.333585960\n",
      "Epoch: 0676 cost= 675.304111000\n",
      "Epoch: 0681 cost= 690.642408411\n",
      "Epoch: 0686 cost= 689.259588581\n",
      "Epoch: 0691 cost= 666.271492312\n",
      "Epoch: 0696 cost= 673.903215916\n",
      "Epoch: 0701 cost= 676.498576273\n",
      "Epoch: 0706 cost= 640.130845106\n",
      "Epoch: 0711 cost= 682.979167505\n",
      "Epoch: 0716 cost= 641.706014590\n",
      "Epoch: 0721 cost= 651.723363256\n",
      "Epoch: 0726 cost= 664.223160492\n",
      "Epoch: 0731 cost= 765.211931108\n",
      "Epoch: 0736 cost= 659.240568144\n",
      "Epoch: 0741 cost= 659.632138110\n",
      "Epoch: 0746 cost= 649.301610998\n",
      "Epoch: 0751 cost= 646.533037227\n",
      "Epoch: 0756 cost= 629.542793874\n",
      "Epoch: 0761 cost= 655.465493852\n",
      "Epoch: 0766 cost= 637.963146129\n",
      "Epoch: 0771 cost= 666.891272933\n",
      "Epoch: 0776 cost= 644.809437594\n",
      "Epoch: 0781 cost= 819.121167265\n",
      "Epoch: 0786 cost= 674.422287445\n",
      "Epoch: 0791 cost= 639.372704757\n",
      "Epoch: 0796 cost= 631.535689313\n",
      "Epoch: 0801 cost= 647.391985881\n",
      "Epoch: 0806 cost= 638.503146244\n",
      "Epoch: 0811 cost= 642.049236775\n",
      "Epoch: 0816 cost= 631.662672039\n",
      "Epoch: 0821 cost= 616.384545057\n",
      "Epoch: 0826 cost= 656.726300837\n",
      "Epoch: 0831 cost= 642.433631939\n",
      "Epoch: 0836 cost= 636.821206439\n",
      "Epoch: 0841 cost= 631.365644218\n",
      "Epoch: 0846 cost= 660.648320873\n",
      "Epoch: 0851 cost= 635.870142300\n",
      "Epoch: 0856 cost= 633.053839340\n",
      "Epoch: 0861 cost= 664.098134924\n",
      "Epoch: 0866 cost= 659.298669731\n",
      "Epoch: 0871 cost= 645.568281110\n",
      "Epoch: 0876 cost= 620.512697264\n",
      "Epoch: 0881 cost= 624.753844749\n",
      "Epoch: 0886 cost= 649.964879575\n",
      "Epoch: 0891 cost= 643.259217274\n",
      "Epoch: 0896 cost= 641.790077533\n",
      "Epoch: 0901 cost= 655.817917393\n",
      "Epoch: 0906 cost= 625.940204349\n",
      "Epoch: 0911 cost= 633.524932834\n",
      "Epoch: 0916 cost= 654.554069493\n",
      "Epoch: 0921 cost= 638.510040213\n",
      "Epoch: 0926 cost= 636.056836254\n",
      "Epoch: 0931 cost= 649.825807464\n",
      "Epoch: 0936 cost= 644.861231818\n",
      "Epoch: 0941 cost= 633.413752130\n",
      "Epoch: 0946 cost= 638.397938940\n",
      "Epoch: 0951 cost= 618.785609443\n",
      "Epoch: 0956 cost= 641.691284939\n",
      "Epoch: 0961 cost= 632.006079639\n",
      "Epoch: 0966 cost= 624.617737651\n",
      "Epoch: 0971 cost= 625.555045839\n",
      "Epoch: 0976 cost= 634.254800102\n",
      "Epoch: 0981 cost= 612.266058828\n",
      "Epoch: 0986 cost= 621.566073088\n",
      "Epoch: 0991 cost= 643.379212239\n",
      "Epoch: 0996 cost= 617.159148850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nChange log: decoder has no dropout or additional tranformations\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "network_architecture = \\\n",
    "    dict(n_hidden_recog_1=100, # 1st layer encoder neurons\n",
    "         n_hidden_recog_2=100, # 2nd layer encoder neurons\n",
    "         n_hidden_gener_1=docs_tr.shape[1], # 1st layer decoder neurons\n",
    "         n_input=docs_tr.shape[1], # MNIST data input (img shape: 28*28)\n",
    "         n_z=50)  # dimensionality of latent space\n",
    "\n",
    "batch_size=200\n",
    "learning_rate=0.002\n",
    "def create_minibatch(data):\n",
    "    rng = np.random.RandomState(10)\n",
    "    \n",
    "    while True:\n",
    "        # Return random data samples of a size 'minibatch_size' at each iteration\n",
    "        ixs = rng.randint(data.shape[0], size=batch_size)\n",
    "        yield data[ixs]\n",
    "        \n",
    "minibatches = create_minibatch(docs_tr.astype('float32'))\n",
    "vae,emb = train(network_architecture, training_epochs=1000,batch_size=batch_size,learning_rate=learning_rate)\n",
    "# plt.plot(cost_plot)\n",
    "\n",
    "\n",
    "'''\n",
    "Change log: decoder has no dropout or additional tranformations\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site anonymous mechanism related usenet\n",
      "budget meeting president meet economy\n",
      "nsa escrow clipper wiretap crypto\n",
      "buy sell dealer cheaper cheap\n",
      "escape serve turks village armenia\n",
      "write article michael anyone andrew\n",
      "clinton libertarian liberty waco arabs\n",
      "necessary maintain right clear individual\n",
      "blood people suffer man die\n",
      "originally general become complete poster\n",
      "regulation means liberty rights state\n",
      "better year last fan hitter\n",
      "armenians troops armenia town brother\n",
      "ready major although particularly mostly\n",
      "clean race ask maintain member\n",
      "help find need use try\n",
      "prevent guarantee state powerful reduce\n",
      "god faith christian heaven pray\n",
      "letter review organize together frequently\n",
      "bike article damn helmet yeah\n",
      "write article get anyone paint\n",
      "cap gary fan mike patrick\n",
      "thanks advance please hus appreciate\n",
      "ide isa hd apple scsi\n",
      "arabs arab waco islamic israeli\n",
      "kid laugh funny poor compound\n",
      "california flight nasa institute activity\n",
      "previously primary facility development additional\n",
      "reach spend area prepare race\n",
      "armenians armenian armenia escape massacre\n",
      "escape town alive dead soldier\n",
      "virtual software unix configuration compatible\n",
      "faq unix implementation extension code\n",
      "environment unix directory bug display\n",
      "thanks please hus advance appreciate\n",
      "god resurrection faith heaven christian\n",
      "sexual compound agree opinion self\n",
      "warning copyright character echo receive\n",
      "pair generate somewhat expensive use\n",
      "adams patrick blue ron ny\n",
      "condition air direction cost equipment\n",
      "university office dc radio june\n",
      "category date submit entry prior\n",
      "mary translation prayer man teach\n",
      "give within clear state intend\n",
      "distribute copyright update ftp environment\n",
      "higher lower bit middle ah\n",
      "phone nsa chip escrow wiretap\n",
      "social establish medicine association transfer\n",
      "sexual definition morality moral sex\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(beta, feature_names, n_top_words=5):\n",
    "    for i in range(len(beta)):\n",
    "        print(\" \".join([feature_names[j]\n",
    "            for j in beta[i].argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "print_top_words(emb, zip(*sorted(vocab.items(), key=lambda x: x[1]))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16199.0 0.116587\n",
      "17675.0 0.109971\n",
      "18777.0 1.26609\n",
      "17442.0 0.110946\n",
      "15009.0 0.123144\n",
      "15909.0 0.118173\n",
      "18313.0 0.107068\n",
      "12130.0 0.14373\n",
      "16855.0 0.113791\n",
      "15659.0 0.11908\n",
      "15464.0 0.119873\n",
      "17277.0 0.111419\n",
      "14149.0 0.128386\n",
      "16921.0 0.113744\n",
      "18644.0 0.106128\n",
      "14920.0 0.123464\n",
      "17499.0 0.110225\n",
      "13722.0 0.130724\n",
      "18477.0 0.106227\n",
      "17703.0 0.110081\n",
      "17120.0 0.112562\n",
      "17441.0 1.54291\n",
      "14657.0 0.124756\n",
      "18285.0 0.106832\n",
      "19759.0 1.36587\n",
      "18497.0 0.106362\n",
      "15005.0 0.12258\n",
      "15945.0 0.117134\n",
      "14745.0 0.124032\n",
      "15818.0 0.11807\n",
      "18630.0 0.105868\n",
      "15471.0 0.120294\n",
      "16040.0 0.117025\n",
      "17460.0 0.115694\n",
      "14946.0 0.123214\n",
      "19375.0 0.103608\n",
      "14271.0 0.127357\n",
      "15567.0 0.119424\n",
      "18622.0 0.105208\n",
      "19180.0 0.105486\n",
      "17573.0 0.109752\n",
      "17800.0 1.51213\n",
      "17002.0 0.112617\n",
      "13314.0 0.134064\n",
      "15109.0 0.122592\n",
      "15292.0 0.121212\n",
      "18751.0 0.106105\n",
      "15352.0 0.120089\n",
      "13684.0 0.131339\n",
      "17678.0 0.109578\n",
      "15344.0 0.120314\n",
      "18686.0 0.105825\n",
      "20209.0 0.0999587\n",
      "15927.0 0.118681\n",
      "14833.0 0.123994\n",
      "13999.0 0.128813\n",
      "1.23346\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cost=[]\n",
    "minibatches = create_minibatch(docs_te.astype('float32')[:7392])\n",
    "total_batch = int(n_samples_tr / batch_size)\n",
    "# Loop over all batches\n",
    "for i in range(total_batch):\n",
    "    batch_xs = minibatches.next()\n",
    "    c=vae.test(batch_xs)\n",
    "    n_d = np.sum(batch_xs)\n",
    "    print n_d,c/n_d\n",
    "    cost.append(c/n_d)\n",
    "    \n",
    "print (np.exp(np.mean(np.array(cost))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('nips.txt','w') as o:\n",
    "\n",
    "    for doc in docs_tr:\n",
    "        doc_o=[]\n",
    "        j=-1\n",
    "        for wc in doc:\n",
    "            j+=1\n",
    "            for i in range(int(wc)):\n",
    "                doc_o.append(vocab[j].strip())\n",
    "        o.write(\" \".join(doc_o)+'\\n')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(cost,open('cost','w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12524\n"
     ]
    }
   ],
   "source": [
    "print (np.exp(np.mean(np.array(cost))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13918528, 0.43915969, 185.49721, 0.18895037, 0.17840242]\n"
     ]
    }
   ],
   "source": [
    "print cost[51:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
