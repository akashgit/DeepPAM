{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/group/cup/data1/akash_itm/IPC_MAP/lib/python2.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools,time\n",
    "import sys, os\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_tr = '/afs/inf.ed.ac.uk/group/cup/data1/akash_itm\\\n",
    "/IPC_MAP/NTP/tensorflow_lda/lda-tf_new/data/20news_clean/train.txt.npy'\n",
    "data_tr = np.load(dataset_tr)\n",
    "dataset_te = '/afs/inf.ed.ac.uk/group/cup/data1/akash_itm\\\n",
    "/IPC_MAP/NTP/tensorflow_lda/lda-tf_new/data/20news_clean/test.txt.npy'\n",
    "data_te = np.load(dataset_te)\n",
    "vocab = '/afs/inf.ed.ac.uk/group/cup/data1/akash_itm\\\n",
    "/IPC_MAP/NTP/tensorflow_lda/lda-tf_new/data/20news_clean/vocab.pkl'\n",
    "vocab = pickle.load(open(vocab,'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11258, 1995)\n",
      "(7487, 1995)\n"
     ]
    }
   ],
   "source": [
    "vocab_size=len(vocab)\n",
    "def onehot(data, min_length):\n",
    "    return np.bincount(data, minlength=min_length)\n",
    "data_tr = np.array([onehot(doc.astype('int'),vocab_size) for doc in data_tr if np.sum(doc)!=0])\n",
    "data_te = np.array([onehot(doc.astype('int'),vocab_size) for doc in data_te if np.sum(doc)!=0])\n",
    "print data_tr.shape\n",
    "print data_te.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples_tr = 11258\n",
    "n_samples_te = 7488\n",
    "docs_tr = data_tr\n",
    "docs_te = data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def xavier_init(fan_in, fan_out, constant=1): \n",
    "    \"\"\" Xavier initialization of network weights\"\"\"\n",
    "    # https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "    low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "    high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out), \n",
    "                             minval=low, maxval=high, \n",
    "                             dtype=tf.float32)\n",
    "def log_dir_init(fan_in, fan_out,topics=50): \n",
    "    return tf.log((1.0/topics)*tf.ones([fan_in, fan_out]))\n",
    "\n",
    "tf.reset_default_graph()\n",
    "class VariationalAutoencoder(object):\n",
    "    \"\"\" \n",
    "    See \"Auto-Encoding Variational Bayes\" by Kingma and Welling for more details.\n",
    "    \"\"\"\n",
    "    def __init__(self, network_architecture, transfer_fct=tf.nn.softplus, \n",
    "                 learning_rate=0.001, batch_size=100):\n",
    "        self.network_architecture = network_architecture\n",
    "        self.transfer_fct = transfer_fct\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        print 'Learning Rate:', self.learning_rate\n",
    "        \n",
    "        # tf Graph input\n",
    "        self.x = tf.placeholder(tf.float32, [None, network_architecture[\"n_input\"]])\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        self.is_training = tf.placeholder(tf.bool, [], name='is_training')\n",
    "        self.samples = tf.placeholder(tf.int32)\n",
    "        \n",
    "        self.h_dim = float(network_architecture[\"n_z\"])\n",
    "        self.a0 = np.ones((1 , 2)).astype(np.float32)/2\n",
    "        self.a1 = np.ones((1 , self.h_dim)).astype(np.float32)/self.h_dim\n",
    "        self.a2 = np.ones((1 , self.h_dim)).astype(np.float32)/self.h_dim\n",
    "        \n",
    "        self.mu2a0 = tf.constant((np.log(self.a0).T-np.mean(np.log(self.a0),1)).T)\n",
    "        self.var2a0 = tf.constant(  ( ( (1.0/self.a0)*( 1 - (2.0/2.0) ) ).T +\n",
    "                                ( 1.0/(self.h_dim*self.h_dim) )*np.sum(1.0/self.a0,1) ).T  )\n",
    "        \n",
    "        self.mu2a1 = tf.constant((np.log(self.a1).T-np.mean(np.log(self.a1),1)).T)\n",
    "        self.var2a1 = tf.constant(  ( ( (1.0/self.a1)*( 1 - (2.0/self.h_dim) ) ).T +\n",
    "                                ( 1.0/(self.h_dim*self.h_dim) )*np.sum(1.0/self.a1,1) ).T  )\n",
    "        \n",
    "        self.mu2a2 = tf.constant((np.log(self.a2).T-np.mean(np.log(self.a2),1)).T)\n",
    "        self.var2a2 = tf.constant(  ( ( (1.0/self.a2)*( 1 - (2.0/self.h_dim) ) ).T +\n",
    "                                ( 1.0/(self.h_dim*self.h_dim) )*np.sum(1.0/self.a2,1) ).T  )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Create autoencoder network\n",
    "        self._create_network()\n",
    "        # Define loss function based variational upper-bound and \n",
    "        # corresponding optimizer\n",
    "        self._create_loss_optimizer()\n",
    "        \n",
    "        # Initializing the tensor flow variables\n",
    "        init = tf.initialize_all_variables()\n",
    "\n",
    "        # Launch the session\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(init)\n",
    "    \n",
    "    def _create_network(self):\n",
    "        # Initialize autoencode network weights and biases\n",
    "        self.network_weights = self._initialize_weights(**self.network_architecture)\n",
    "        '''Adding 2 subtopics and 1 super topic.'''\n",
    "        self.z_mean0,self.z_log_sigma_sq0 = \\\n",
    "            self._recognition_network(self.network_weights[\"weights_recog\"], \n",
    "                                      self.network_weights[\"biases_recog\"])\n",
    "            \n",
    "        self.z_mean1,self.z_log_sigma_sq1 = \\\n",
    "            self._recognition_network(self.network_weights[\"sub_01\"], \n",
    "                                      self.network_weights[\"biases_sub_01\"])\n",
    "        \n",
    "        self.z_mean2,self.z_log_sigma_sq2 = \\\n",
    "            self._recognition_network(self.network_weights[\"sub_01\"], \n",
    "                                      self.network_weights[\"biases_sub_01\"])\n",
    "\n",
    "        # Draw one sample z from Gaussian distribution\n",
    "        '''Adding another noise variable for super toipc: hard coding atm'''\n",
    "        eps0 = tf.random_normal((self.samples, 2), 0, 1, \n",
    "                               dtype=tf.float32)\n",
    "        n_z = self.network_architecture[\"n_z\"]\n",
    "        eps1 = tf.random_normal((self.samples, n_z), 0, 1, \n",
    "                               dtype=tf.float32)\n",
    "        eps2 = tf.random_normal((self.samples, n_z), 0, 1, \n",
    "                               dtype=tf.float32)\n",
    "        '''Adding RT for subtopics'''\n",
    "        self.z0 = tf.nn.softmax(tf.contrib.layers.batch_norm(tf.add(self.z_mean0, \n",
    "                        tf.mul(tf.sqrt(tf.exp(self.z_log_sigma_sq0)), eps0))))\n",
    "        self.z0 = tf.nn.softmax(tf.contrib.layers.batch_norm(self.z0,\n",
    "                                                             is_training=self.is_training,updates_collections=None))\n",
    "        \n",
    "        self.sigma0 = tf.exp(self.z_log_sigma_sq0)\n",
    "        \n",
    "        self.z1 = tf.add(self.z_mean1, \n",
    "                        tf.mul(tf.sqrt(tf.exp(self.z_log_sigma_sq1)), eps1))\n",
    "        self.z1 = tf.nn.softmax(tf.contrib.layers.batch_norm(self.z1,\n",
    "                                                            is_training=self.is_training,updates_collections=None))\n",
    "        \n",
    "        self.sigma1 = tf.exp(self.z_log_sigma_sq1)\n",
    "        \n",
    "        self.z2 = tf.add(self.z_mean2, \n",
    "                        tf.mul(tf.sqrt(tf.exp(self.z_log_sigma_sq2)), eps2))\n",
    "        self.z2 = tf.nn.softmax(tf.contrib.layers.batch_norm(self.z2,\n",
    "                                                            is_training=self.is_training,updates_collections=None))\n",
    "        \n",
    "        self.sigma2 = tf.exp(self.z_log_sigma_sq2)\n",
    "        \n",
    "        '''Adding subtopic reconstruction'''\n",
    "            \n",
    "#         self.theta = tf.matmul(self.z0,tf.concat(0,[self.z1, self.z2]))\n",
    "        \n",
    "        self.theta = tf.squeeze(tf.map_fn(lambda i: \n",
    "                                          tf.matmul(tf.expand_dims(tf.gather(self.z0,i),0),\n",
    "                                                    tf.concat(0,[tf.expand_dims(tf.gather(self.z1,i),0),\n",
    "                                                                tf.expand_dims(tf.gather(self.z2,i),0)])),\n",
    "                                                   tf.constant(np.arange(self.batch_size)), dtype=tf.float32))\n",
    "            \n",
    "        self.x_reconstr_mean = \\\n",
    "            self._generator_network(tf.nn.dropout(self.theta, self.keep_prob),\n",
    "                                    self.network_weights[\"weights_gener\"],\n",
    "                                    self.network_weights[\"biases_gener\"])\n",
    "            \n",
    "    def _initialize_weights(self, n_hidden_recog_1, n_hidden_recog_2, \n",
    "                            n_hidden_gener_1,  \n",
    "                            n_input, n_z):\n",
    "        all_weights = dict()\n",
    "        with tf.variable_scope(\"super\"):\n",
    "            all_weights['weights_recog'] = {\n",
    "                'h1': tf.get_variable('h1',[n_input, n_hidden_recog_1]),\n",
    "                'h2': tf.get_variable('h2',[n_hidden_recog_1, n_hidden_recog_2]),\n",
    "                'out_mean': tf.get_variable('out_mean',[n_hidden_recog_2, 2]),\n",
    "                'out_log_sigma': tf.get_variable('out_log_sigma',[n_hidden_recog_2, 2])\n",
    "            }\n",
    "            all_weights['biases_recog'] = {\n",
    "                'b1': tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32)),\n",
    "                'b2': tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32)),\n",
    "                'out_mean': tf.Variable(tf.zeros([2], dtype=tf.float32)),\n",
    "                'out_log_sigma': tf.Variable(tf.zeros([2], dtype=tf.float32))\n",
    "            }\n",
    "        with tf.variable_scope(\"sub01\"):\n",
    "            all_weights['sub_01'] = {\n",
    "                'h1': tf.get_variable('h1',[n_input, n_hidden_recog_1]),\n",
    "                'h2': tf.get_variable('h2',[n_hidden_recog_1, n_hidden_recog_2]),\n",
    "                'out_mean': tf.get_variable('out_mean',[n_hidden_recog_2, n_z]),\n",
    "                'out_log_sigma': tf.get_variable('out_log_sigma',[n_hidden_recog_2, n_z])}\n",
    "            all_weights['biases_sub_01'] = {\n",
    "                'b1': tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32)),\n",
    "                'b2': tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32)),\n",
    "                'out_mean': tf.Variable(tf.zeros([n_z], dtype=tf.float32)),\n",
    "                'out_log_sigma': tf.Variable(tf.zeros([n_z], dtype=tf.float32))\n",
    "            }\n",
    "        with tf.variable_scope(\"sub02\"):\n",
    "            all_weights['sub_02'] = {\n",
    "                'h1': tf.get_variable('h1',[n_input, n_hidden_recog_1]),\n",
    "                'h2': tf.get_variable('h2',[n_hidden_recog_1, n_hidden_recog_2]),\n",
    "                'out_mean': tf.get_variable('out_mean',[n_hidden_recog_2, n_z]),\n",
    "                'out_log_sigma': tf.get_variable('out_log_sigma',[n_hidden_recog_2, n_z])\n",
    "            }\n",
    "            all_weights['biases_sub_02'] = {\n",
    "                'b1': tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32)),\n",
    "                'b2': tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32)),\n",
    "                'out_mean': tf.Variable(tf.zeros([n_z], dtype=tf.float32)),\n",
    "                'out_log_sigma': tf.Variable(tf.zeros([n_z], dtype=tf.float32))\n",
    "            }\n",
    "        with tf.variable_scope(\"gen\"):\n",
    "            all_weights['weights_gener'] = {\n",
    "                'h2': tf.Variable(xavier_init(n_z, n_hidden_gener_1))\n",
    "            }\n",
    "            all_weights['biases_gener'] = {\n",
    "                'b2': tf.Variable(tf.zeros([n_hidden_gener_1], dtype=tf.float32))\n",
    "            }\n",
    "        return all_weights\n",
    "            \n",
    "    def _recognition_network(self, weights, biases):\n",
    "        # Generate probabilistic encoder (recognition network)\n",
    "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.x, weights['h1']), \n",
    "                                           biases['b1'])) \n",
    "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
    "                                           biases['b2'])) \n",
    "        layer_do = tf.nn.dropout(layer_2, self.keep_prob)\n",
    "        \n",
    "        z_mean = tf.contrib.layers.batch_norm(tf.add(tf.matmul(layer_do, weights['out_mean']),\n",
    "                        biases['out_mean']),is_training=self.is_training,updates_collections=None)\n",
    "        z_log_sigma_sq = \\\n",
    "            tf.contrib.layers.batch_norm(tf.add(tf.matmul(layer_do, weights['out_log_sigma']), \n",
    "                   biases['out_log_sigma']),is_training=self.is_training,updates_collections=None)     \n",
    "        \n",
    "        return (z_mean, z_log_sigma_sq)\n",
    "\n",
    "    def _generator_network(self,z, weights, biases):\n",
    "        # Generate probabilistic decoder (decoder network)\n",
    "#         self.layer_do_0 = tf.nn.dropout(tf.nn.softmax(tf.contrib.layers.batch_norm(z)), self.keep_prob)\n",
    "        self.layer_do_0 =z\n",
    "        x_reconstr_mean = tf.add(tf.matmul(self.layer_do_0, \n",
    "                                           tf.nn.softmax(tf.contrib.layers.batch_norm(weights['h2'],\\\n",
    "is_training=self.is_training,updates_collections=None))),0.0)\n",
    "#         x_reconstr_mean = tf.nn.softmax(tf.contrib.layers.batch_norm(tf.add(\n",
    "#                     tf.matmul(self.layer_do_0, weights['h2']),0.0)))\n",
    "    \n",
    "        return x_reconstr_mean\n",
    "\n",
    "            \n",
    "    def _create_loss_optimizer(self):\n",
    "        \n",
    "        self.x_reconstr_mean+=1e-10\n",
    "     \n",
    "        reconstr_loss = \\\n",
    "            -tf.reduce_sum(self.x * tf.log(self.x_reconstr_mean),1)#/tf.reduce_sum(self.x,1) \n",
    "\n",
    "    \n",
    "        latent_loss0 = 0.5*( tf.reduce_sum(tf.div(self.sigma0,self.var2a0),1)+\\\n",
    "        tf.reduce_sum( tf.mul(tf.div((self.mu2a0 - self.z_mean0),self.var2a0),\n",
    "                  (self.mu2a0 - self.z_mean0)),1) - 2 +\\\n",
    "                           tf.reduce_sum(tf.log(self.var2a0),1)  - tf.reduce_sum(self.z_log_sigma_sq0  ,1) ) \n",
    "        \n",
    "        latent_loss1 = 0.5*( tf.reduce_sum(tf.div(self.sigma1,self.var2a1),1)+\\\n",
    "        tf.reduce_sum( tf.mul(tf.div((self.mu2a1 - self.z_mean1),self.var2a1),\n",
    "                  (self.mu2a1 - self.z_mean1)),1) - self.h_dim +\\\n",
    "                           tf.reduce_sum(tf.log(self.var2a1),1)  - tf.reduce_sum(self.z_log_sigma_sq1  ,1) )\n",
    "        \n",
    "        latent_loss2 = 0.5*( tf.reduce_sum(tf.div(self.sigma2,self.var2a2),1)+\\\n",
    "        tf.reduce_sum( tf.mul(tf.div((self.mu2a2 - self.z_mean2),self.var2a2),\n",
    "                  (self.mu2a2 - self.z_mean2)),1) - self.h_dim +\\\n",
    "                           tf.reduce_sum(tf.log(self.var2a2),1)  - tf.reduce_sum(self.z_log_sigma_sq2  ,1) )\n",
    "\n",
    "        \n",
    "        \n",
    "#         latent_loss = -0.5 * tf.reduce_sum(1 + self.z_log_sigma_sq \n",
    "#                                            - tf.square(self.z_mean) \n",
    "#                                            - tf.exp(self.z_log_sigma_sq), 1)\n",
    "\n",
    "\n",
    "        self.cost = tf.reduce_mean(reconstr_loss) \\\n",
    "    + tf.reduce_mean(latent_loss0 + latent_loss1 + latent_loss2) # average over batch\n",
    "        \n",
    "        # Use ADAM optimizer\n",
    "        self.optimizer = \\\n",
    "            tf.train.AdamOptimizer(learning_rate=self.learning_rate,beta1=0.9).minimize(self.cost)\n",
    "        \n",
    "    def partial_fit(self, X):\n",
    "        \"\"\"Train model based on mini-batch of input data.\n",
    "        \n",
    "        Return cost of mini-batch.\n",
    "        \"\"\"\n",
    "#         opt, cost,emb,kld = self.sess.run((self.optimizer, self.cost,self.network_weights['weights_gener']['h2'],\n",
    "#                                           self.kld),feed_dict={self.x: X,self.keep_prob: .9})\n",
    "        opt,cost,emb = self.sess.run((self.optimizer, self.cost,self.network_weights['weights_gener']['h2']\n",
    "                                      ),feed_dict={self.x: X,self.keep_prob: .7,self.is_training:True,self.samples:1})\n",
    "        return cost,emb\n",
    "    \n",
    "    def test(self, X):\n",
    "        \"\"\"Test the model and return the lowerbound on the log-likelihood.\n",
    "        \"\"\"\n",
    "        cost,e1,e2 = self.sess.run((self.cost,self.z1,self.z2),\n",
    "                                        feed_dict={self.x: np.expand_dims(X, axis=0),self.keep_prob: 1.0,\n",
    "                                                   self.is_training:False,self.samples:1})\n",
    "        return cost,e1,e2\n",
    "    \n",
    "    def batchtest(self, X):\n",
    "        \"\"\"Test the model and return the lowerbound on the log-likelihood.\n",
    "        \"\"\"\n",
    "        cost,e1,e2 = self.sess.run((self.cost,self.z1,self.z2),\n",
    "                                        feed_dict={self.x: X,self.keep_prob: 1.0,\n",
    "                                                   self.is_training:False,self.samples:1})\n",
    "        return cost,e1,e2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "cost_plot=[]\n",
    "test_batch = create_minibatch(docs_tr.astype('float32'))\n",
    "def train(network_architecture, learning_rate=0.0001,\n",
    "          batch_size=100, training_epochs=10, display_step=5):\n",
    "    vae = VariationalAutoencoder(network_architecture, \n",
    "                                 learning_rate=learning_rate, \n",
    "                                 batch_size=batch_size)\n",
    "    emb=0\n",
    "    avg_kld=0\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        avg_kld=0.\n",
    "        total_batch = int(n_samples_tr / batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs = minibatches.next()\n",
    "            # Fit training using batch data\n",
    "            cost,emb = vae.partial_fit(batch_xs)\n",
    "            c,a,b = vae.batchtest(batch_xs)\n",
    "\n",
    "#             sys.exit()\n",
    "            # Compute average loss\n",
    "            avg_cost += cost / n_samples_tr * batch_size\n",
    "            avg_kld += c / n_samples_tr * batch_size\n",
    "            \n",
    "            if np.isnan(avg_cost):\n",
    "                print epoch,i,np.sum(batch_xs,1).astype(np.int),batch_xs.shape\n",
    "                return vae,emb\n",
    "#                 sys.exit()\n",
    "        \n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            cost_plot.append(avg_cost)\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \\\n",
    "                  \"cost=\", \"{:.9f}\".format(avg_cost)\n",
    "            print 'avg_cost = ',avg_kld\n",
    "            test_c=0.\n",
    "            for i in range(int(n_samples_te / batch_size)):\n",
    "                c,a,b = vae.batchtest(test_batch.next())\n",
    "                test_c+=c / n_samples_te * batch_size\n",
    "            print 'avg_test_cost = ',test_c\n",
    "    return vae,emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.001\n",
      "Epoch: 0001 cost= 3318.603450567\n",
      "avg_cost =  12591126.1308\n",
      "avg_test_cost =  2019.43181356\n",
      "Epoch: 0006 cost= 2053.020929041\n",
      "avg_cost =  1716.22363463\n",
      "avg_test_cost =  1701.8334674\n",
      "Epoch: 0011 cost= 1916.104123266\n",
      "avg_cost =  2274.87858677\n",
      "avg_test_cost =  1535.79411955\n",
      "Epoch: 0016 cost= 1859.198265869\n",
      "avg_cost =  1445.96102537\n",
      "avg_test_cost =  1389.72198617\n",
      "Epoch: 0021 cost= 1790.077184263\n",
      "avg_cost =  1434.62223738\n",
      "avg_test_cost =  1368.37682479\n",
      "Epoch: 0026 cost= 1739.282517154\n",
      "avg_cost =  2061.98986363\n",
      "avg_test_cost =  1758.78781376\n",
      "Epoch: 0031 cost= 1588.018051747\n",
      "avg_cost =  1232.1025578\n",
      "avg_test_cost =  1191.68164506\n",
      "Epoch: 0036 cost= 1674.408983646\n",
      "avg_cost =  2076.73804791\n",
      "avg_test_cost =  1668.26164539\n",
      "Epoch: 0041 cost= 1597.443972567\n",
      "avg_cost =  1445.41125681\n",
      "avg_test_cost =  1622.31872917\n",
      "Epoch: 0046 cost= 1542.420632828\n",
      "avg_cost =  1268.53173941\n",
      "avg_test_cost =  1320.7271038\n",
      "Epoch: 0051 cost= 1527.893170499\n",
      "avg_cost =  1495.68898358\n",
      "avg_test_cost =  1478.2509926\n",
      "Epoch: 0056 cost= 1433.320129470\n",
      "avg_cost =  1079.86885216\n",
      "avg_test_cost =  1189.89803119\n",
      "Epoch: 0061 cost= 1355.367044980\n",
      "avg_cost =  1060.92747132\n",
      "avg_test_cost =  1014.95902396\n",
      "Epoch: 0066 cost= 1228.707261728\n",
      "avg_cost =  1034.56595145\n",
      "avg_test_cost =  1066.62828657\n",
      "Epoch: 0071 cost= 913.879076832\n",
      "avg_cost =  1032.33392105\n",
      "avg_test_cost =  1004.67039418\n",
      "Epoch: 0076 cost= 855.365822976\n",
      "avg_cost =  993.323883745\n",
      "avg_test_cost =  968.089723179\n",
      "Epoch: 0081 cost= 829.717636989\n",
      "avg_cost =  994.243557446\n",
      "avg_test_cost =  971.598038307\n",
      "Epoch: 0086 cost= 989.249140281\n",
      "avg_cost =  927.125022076\n",
      "avg_test_cost =  987.203007886\n",
      "Epoch: 0091 cost= 863.945678776\n",
      "avg_cost =  1063.33049124\n",
      "avg_test_cost =  1022.18639504\n",
      "Epoch: 0096 cost= 799.339547346\n",
      "avg_cost =  1053.31878255\n",
      "avg_test_cost =  1056.70890156\n",
      "Epoch: 0101 cost= 813.426586103\n",
      "avg_cost =  1026.2232946\n",
      "avg_test_cost =  987.31181805\n",
      "Epoch: 0106 cost= 788.066053636\n",
      "avg_cost =  982.186429364\n",
      "avg_test_cost =  958.414822766\n",
      "Epoch: 0111 cost= 737.881092933\n",
      "avg_cost =  924.177355825\n",
      "avg_test_cost =  920.392422798\n",
      "Epoch: 0116 cost= 758.090943489\n",
      "avg_cost =  898.320829493\n",
      "avg_test_cost =  886.638859806\n",
      "Epoch: 0121 cost= 707.153675962\n",
      "avg_cost =  843.245647279\n",
      "avg_test_cost =  852.513874087\n",
      "Epoch: 0126 cost= 721.288368549\n",
      "avg_cost =  838.038055967\n",
      "avg_test_cost =  847.408015911\n",
      "Epoch: 0131 cost= 713.661254297\n",
      "avg_cost =  806.201888596\n",
      "avg_test_cost =  789.465442886\n",
      "Epoch: 0136 cost= 695.541461532\n",
      "avg_cost =  787.113119082\n",
      "avg_test_cost =  801.977199978\n",
      "Epoch: 0141 cost= 700.222735491\n",
      "avg_cost =  832.481085282\n",
      "avg_test_cost =  860.853909224\n",
      "Epoch: 0146 cost= 685.890964082\n",
      "avg_cost =  817.475601335\n",
      "avg_test_cost =  766.196725307\n",
      "Epoch: 0151 cost= 683.528333194\n",
      "avg_cost =  783.41821185\n",
      "avg_test_cost =  782.864075033\n",
      "Epoch: 0156 cost= 665.568374685\n",
      "avg_cost =  825.445636879\n",
      "avg_test_cost =  832.168420971\n",
      "Epoch: 0161 cost= 657.049682530\n",
      "avg_cost =  812.933192386\n",
      "avg_test_cost =  798.111718333\n",
      "Epoch: 0166 cost= 640.798746889\n",
      "avg_cost =  849.524819243\n",
      "avg_test_cost =  843.381206806\n",
      "Epoch: 0171 cost= 669.055789494\n",
      "avg_cost =  770.641282041\n",
      "avg_test_cost =  756.576128903\n",
      "Epoch: 0176 cost= 652.421725150\n",
      "avg_cost =  707.154471837\n",
      "avg_test_cost =  670.401441134\n",
      "Epoch: 0181 cost= 642.248670694\n",
      "avg_cost =  755.529150842\n",
      "avg_test_cost =  727.395424476\n",
      "Epoch: 0186 cost= 643.882040436\n",
      "avg_cost =  797.716096765\n",
      "avg_test_cost =  808.724093641\n",
      "Epoch: 0191 cost= 628.250607294\n",
      "avg_cost =  780.229721842\n",
      "avg_test_cost =  824.331009694\n",
      "Epoch: 0196 cost= 631.010261084\n",
      "avg_cost =  703.342086296\n",
      "avg_test_cost =  655.235363887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nChange log: decoder has no dropout or additional tranformations\\n'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "network_architecture = \\\n",
    "    dict(n_hidden_recog_1=100, # 1st layer encoder neurons\n",
    "         n_hidden_recog_2=100, # 2nd layer encoder neurons\n",
    "         n_hidden_gener_1=1995, # 1st layer decoder neurons\n",
    "         n_input=1995, # MNIST data input (img shape: 28*28)\n",
    "         n_z=50)  # dimensionality of latent space\n",
    "\n",
    "batch_size=200\n",
    "learning_rate=0.001\n",
    "def create_minibatch(data):\n",
    "    rng = np.random.RandomState(10)\n",
    "    \n",
    "    while True:\n",
    "        # Return random data samples of a size 'minibatch_size' at each iteration\n",
    "        ixs = rng.randint(data.shape[0], size=batch_size)\n",
    "        yield data[ixs]\n",
    "        \n",
    "minibatches = create_minibatch(docs_tr.astype('float32'))\n",
    "vae,emb = train(network_architecture, training_epochs=200,batch_size=batch_size,learning_rate=learning_rate)\n",
    "# plt.plot(cost_plot)\n",
    "\n",
    "\n",
    "'''\n",
    "Change log: decoder has no dropout or additional tranformations\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "motherboard card video upgrade port\n",
      "god christian jesus bible faith\n",
      "michael friend picture mike disclaimer\n",
      "space laboratory datum nasa shuttle\n",
      "israeli jew israel homosexual jewish\n",
      "god christian believe keith jon\n",
      "surrender compound waco gordon gas\n",
      "science brain treatment msg scientific\n",
      "thanks hus video anyone driver\n",
      "space flight orbit spacecraft nasa\n",
      "chip key phone clipper encryption\n",
      "armenians kill witness soul turks\n",
      "pat new university ron june\n",
      "unix user software ftp information\n",
      "volume active identify overall disease\n",
      "exist existence prove atheist assumption\n",
      "fax email mail address please\n",
      "signal electronics tube amp voltage\n",
      "go say know think come\n",
      "rangers toronto montreal pittsburgh game\n",
      "government law federal illegal court\n",
      "windows memory microsoft printer nt\n",
      "game team playoff score rangers\n",
      "write think like yeah get\n",
      "god christian bible believe faith\n",
      "program file window display format\n",
      "thanks please anyone hus appreciate\n",
      "gun clinton criminal texas tax\n",
      "university institute april research california\n",
      "bike ride car write fun\n",
      "motherboard hd mb card adapter\n",
      "key phone chip clipper escrow\n",
      "fuel air heat frequency cost\n",
      "thanks please anyone appreciate hus\n",
      "april congress states united fund\n",
      "player baseball game team fan\n",
      "reaction cause eat msg brain\n",
      "law rights government country citizen\n",
      "write article gordon gmt ken\n",
      "kid rock cop shoot fire\n",
      "armenia israeli armenians armenian israel\n",
      "people country make gay say\n",
      "kid complain funny responsible rich\n",
      "baseball year player pitch fan\n",
      "write anyone article get like\n",
      "window screen font button windows\n",
      "sale shipping manual price speaker\n",
      "bike car honda ride engine\n",
      "program file format image input\n",
      "engine honda bike car battery\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(beta, feature_names, n_top_words=5):\n",
    "    for i in range(len(beta)):\n",
    "        print(\" \".join([feature_names[j]\n",
    "            for j in beta[i].argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "print_top_words(emb, zip(*sorted(vocab.items(), key=lambda x: x[1]))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "2013.83\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cost=[]\n",
    "e1=[]\n",
    "e2=[]\n",
    "idx=0\n",
    "for doc in docs_te:\n",
    "    idx+=1\n",
    "    doc = doc.astype('float32')\n",
    "    n_d = np.sum(doc)\n",
    "    c,a,b=vae.test(doc)\n",
    "    cost.append(c)\n",
    "#     print c,n_d\n",
    "    e1.append(a)\n",
    "    e2.append(b)\n",
    "    if idx%1000==0:\n",
    "        print idx\n",
    "    \n",
    "print ((np.mean(np.array(cost))))\n",
    "# print np.mean(e1,0)\n",
    "# print np.mean(e2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14  2 18 42 12 28 46 31 10  6  9 15 17  3 41 22 13 19 48  7 25 32 30 40 21\n",
      " 34 43 20 37 39 16 47 27 24  0 11  4 45 49 38 35  5 36  1 44 33 23 26 29  8]\n",
      "[14 42  2 15 10 18 46 12  6 40  9 30 38 45 32 41 39 34 11  7 28  3 36  5 31\n",
      " 27 19 16 48 17 37 35 13  4 20 21  0 47 25 49 43 33 24 22  1  8 26 29 23 44]\n",
      "[[ 0.02194456  0.02537401  0.0108702   0.01782281  0.02246     0.02527412\n",
      "   0.01636191  0.01938766  0.02838152  0.01651694  0.0161326   0.02238633\n",
      "   0.01337486  0.01914852  0.00881824  0.01656394  0.0206736   0.01703456\n",
      "   0.01097411  0.01922328  0.02026629  0.01963202  0.01850399  0.0276175\n",
      "   0.02152281  0.01939556  0.02804637  0.02142353  0.01426757  0.02817168\n",
      "   0.0194367   0.01583602  0.01942176  0.02656861  0.01987723  0.02465023\n",
      "   0.02527926  0.02029347  0.02375558  0.02066274  0.01956308  0.01800217\n",
      "   0.01165098  0.02009346  0.02611873  0.02261652  0.01547787  0.02099394\n",
      "   0.01933608  0.02278742]]\n",
      "[[ 0.0223718   0.0263685   0.01101574  0.01919817  0.02067319  0.01930213\n",
      "   0.01607164  0.01903868  0.02837154  0.0169462   0.01389845  0.01886544\n",
      "   0.0156224   0.02058694  0.01016498  0.01383855  0.01968745  0.01995297\n",
      "   0.01527045  0.0194461   0.02169614  0.0218575   0.02457069  0.03028064\n",
      "   0.02415491  0.02259257  0.03005351  0.01937589  0.01910869  0.03007068\n",
      "   0.0175001   0.01935648  0.01767831  0.0235436   0.01817714  0.02023406\n",
      "   0.01926289  0.02007915  0.01751648  0.01791637  0.01610227  0.01774503\n",
      "   0.01077493  0.0229008   0.03223155  0.01762442  0.01550022  0.02258484\n",
      "   0.01994749  0.02286406]]\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "print np.argsort(np.mean(e1,0)[0])\n",
    "print np.argsort(np.mean(e2,0)[0])\n",
    "print np.mean(e1,0)#[0]\n",
    "print np.mean(e2,0)#[0]\n",
    "print np.exp(np.mean(np.array(cost)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(cost,open('cost','w'))\n",
    "pickle.dump(e1,open('e1','w'))\n",
    "pickle.dump(e2,open('e2','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.066614\n",
      "0.00266949\n"
     ]
    }
   ],
   "source": [
    "e=e2\n",
    "print np.amax(np.mean(e,0))\n",
    "print np.min(np.mean(e,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "37\n",
      "Epoch: 0001 cost= 1951.299849720\n",
      "avg_cost =  681.862591718\n",
      "37\n",
      "Epoch: 0002 cost= 1802.893371419\n",
      "avg_cost =  674.743327373\n",
      "37\n",
      "Epoch: 0003 cost= 2080.210038171\n",
      "avg_cost =  687.620173274\n",
      "37\n",
      "Epoch: 0004 cost= 2131.480287028\n",
      "avg_cost =  689.811478898\n",
      "37\n",
      "Epoch: 0005 cost= 1697.812007509\n",
      "avg_cost =  669.3386082\n",
      "37\n",
      "Epoch: 0006 cost= 1812.600706940\n",
      "avg_cost =  675.226615287\n",
      "37\n",
      "Epoch: 0007 cost= 2147.505452401\n",
      "avg_cost =  690.485597353\n",
      "37\n",
      "Epoch: 0008 cost= 1754.365650309\n",
      "avg_cost =  672.287635597\n",
      "37\n",
      "Epoch: 0009 cost= 1986.632824159\n",
      "avg_cost =  683.477679278\n",
      "37\n",
      "Epoch: 0010 cost= 1301.936128341\n",
      "avg_cost =  645.444698849\n"
     ]
    }
   ],
   "source": [
    "test_batch = create_minibatch(docs_te.astype('float32'))\n",
    "def test(vae,minibatches):\n",
    "    emb=0\n",
    "    avg_kld=0\n",
    "    print batch_size\n",
    "    # Training cycle\n",
    "    for epoch in range(10):\n",
    "        avg_cost = 0.\n",
    "        avg_kld = 0.\n",
    "        total_batch = int(n_samples_te / batch_size)\n",
    "        print total_batch\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs = minibatches.next()\n",
    "            # Fit training using batch data\n",
    "            cost,a,b = vae.batchtest(batch_xs)\n",
    "            cost=cost\n",
    "            # Compute average loss\n",
    "            avg_cost += cost / total_batch#n_samples_tr * batch_size\n",
    "\n",
    "\n",
    "        # Display logs per epoch step\n",
    "#         if epoch % display_step == 0:\n",
    "        cost_plot.append(avg_cost)\n",
    "        print \"Epoch:\", '%04d' % (epoch+1), \\\n",
    "              \"cost=\", \"{:.9f}\".format(np.exp((avg_cost/90)))\n",
    "        print 'avg_cost = ',avg_cost\n",
    "\n",
    "test(vae,test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.0701\n"
     ]
    }
   ],
   "source": [
    "print np.mean(np.sum(docs_tr.astype('float32'),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
