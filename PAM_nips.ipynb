{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools,time\n",
    "import sys, os\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 12419) (12419,)\n"
     ]
    }
   ],
   "source": [
    "dataset_tr = '/afs/inf.ed.ac.uk/group/cup/data1/akash_itm/IPC_MAP/NTP/process_UCI/M_nips.full_docs.mat'\n",
    "data_tr = sp.io.loadmat(dataset_tr)['M'].T.toarray()\n",
    "\n",
    "vocab_ = '/afs/inf.ed.ac.uk/group/cup/data1/akash_itm/IPC_MAP/NTP/process_UCI/vocab.nips.txt'\n",
    "vocab_ = open(vocab_,'r').readlines()\n",
    "vocab=np.array([word for word in vocab_])\n",
    "\n",
    "print data_tr.shape, vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_size=len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples_tr = data_tr.shape[0]\n",
    "docs_tr = data_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def xavier_init(fan_in, fan_out, constant=1): \n",
    "    \"\"\" Xavier initialization of network weights\"\"\"\n",
    "    # https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "    low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "    high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out), \n",
    "                             minval=low, maxval=high, \n",
    "                             dtype=tf.float32)\n",
    "def log_dir_init(fan_in, fan_out,topics=50): \n",
    "    return tf.log((1.0/topics)*tf.ones([fan_in, fan_out]))\n",
    "\n",
    "tf.reset_default_graph()\n",
    "class VariationalAutoencoder(object):\n",
    "    \"\"\" \n",
    "    See \"Auto-Encoding Variational Bayes\" by Kingma and Welling for more details.\n",
    "    \"\"\"\n",
    "    def __init__(self, network_architecture, transfer_fct=tf.nn.softplus, \n",
    "                 learning_rate=0.001, batch_size=100):\n",
    "        self.network_architecture = network_architecture\n",
    "        self.transfer_fct = transfer_fct\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        print 'Learning Rate:', self.learning_rate\n",
    "        \n",
    "        # tf Graph input\n",
    "        self.x = tf.placeholder(tf.float32, [None, network_architecture[\"n_input\"]])\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        self.h_dim = float(network_architecture[\"n_z\"])\n",
    "        self.a0 = np.ones((1 , 2)).astype(np.float32)\n",
    "        self.a1 = np.ones((1 , self.h_dim)).astype(np.float32)\n",
    "        self.a2 = np.ones((1 , self.h_dim)).astype(np.float32)/self.h_dim\n",
    "        \n",
    "        self.mu2a0 = tf.constant((np.log(self.a0).T-np.mean(np.log(self.a0),1)).T)\n",
    "        self.var2a0 = tf.constant(  ( ( (1.0/self.a0)*( 1 - (2.0/2.0) ) ).T +\n",
    "                                ( 1.0/(self.h_dim*self.h_dim) )*np.sum(1.0/self.a0,1) ).T  )\n",
    "        \n",
    "        self.mu2a1 = tf.constant((np.log(self.a1).T-np.mean(np.log(self.a1),1)).T)\n",
    "        self.var2a1 = tf.constant(  ( ( (1.0/self.a1)*( 1 - (2.0/self.h_dim) ) ).T +\n",
    "                                ( 1.0/(self.h_dim*self.h_dim) )*np.sum(1.0/self.a1,1) ).T  )\n",
    "        \n",
    "        self.mu2a2 = tf.constant((np.log(self.a2).T-np.mean(np.log(self.a2),1)).T)\n",
    "        self.var2a2 = tf.constant(  ( ( (1.0/self.a2)*( 1 - (2.0/self.h_dim) ) ).T +\n",
    "                                ( 1.0/(self.h_dim*self.h_dim) )*np.sum(1.0/self.a2,1) ).T  )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Create autoencoder network\n",
    "        self._create_network()\n",
    "        # Define loss function based variational upper-bound and \n",
    "        # corresponding optimizer\n",
    "        self._create_loss_optimizer()\n",
    "        \n",
    "        # Initializing the tensor flow variables\n",
    "        init = tf.initialize_all_variables()\n",
    "\n",
    "        # Launch the session\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(init)\n",
    "    \n",
    "    def _create_network(self):\n",
    "        # Initialize autoencode network weights and biases\n",
    "        self.network_weights = self._initialize_weights(**self.network_architecture)\n",
    "        '''Adding 2 subtopics and 1 super topic.'''\n",
    "        self.z_mean0,self.z_log_sigma_sq0 = \\\n",
    "            self._recognition_network(self.network_weights[\"weights_recog\"], \n",
    "                                      self.network_weights[\"biases_recog\"])\n",
    "            \n",
    "        self.z_mean1,self.z_log_sigma_sq1 = \\\n",
    "            self._recognition_network(self.network_weights[\"sub_01\"], \n",
    "                                      self.network_weights[\"biases_sub_01\"])\n",
    "        \n",
    "        self.z_mean2,self.z_log_sigma_sq2 = \\\n",
    "            self._recognition_network(self.network_weights[\"sub_01\"], \n",
    "                                      self.network_weights[\"biases_sub_01\"])\n",
    "\n",
    "        # Draw one sample z from Gaussian distribution\n",
    "        '''Adding another noise variable for super toipc: hard coding atm'''\n",
    "        eps0 = tf.random_normal((1, 2), 0, 1, \n",
    "                               dtype=tf.float32)\n",
    "        n_z = self.network_architecture[\"n_z\"]\n",
    "        eps1 = tf.random_normal((1, n_z), 0, 1, \n",
    "                               dtype=tf.float32)\n",
    "        eps2 = tf.random_normal((1, n_z), 0, 1, \n",
    "                               dtype=tf.float32)\n",
    "        '''Adding RT for subtopics'''\n",
    "        self.z0 = tf.nn.softmax(tf.contrib.layers.batch_norm(tf.add(self.z_mean0, \n",
    "                        tf.mul(tf.sqrt(tf.exp(self.z_log_sigma_sq0)), eps0))))\n",
    "        self.sigma0 = tf.exp(self.z_log_sigma_sq0)\n",
    "        \n",
    "        self.z1 = tf.add(self.z_mean1, \n",
    "                        tf.mul(tf.sqrt(tf.exp(self.z_log_sigma_sq1)), eps1))\n",
    "        self.sigma1 = tf.exp(self.z_log_sigma_sq1)\n",
    "        \n",
    "        self.z2 = tf.add(self.z_mean2, \n",
    "                        tf.mul(tf.sqrt(tf.exp(self.z_log_sigma_sq2)), eps2))\n",
    "        self.sigma2 = tf.exp(self.z_log_sigma_sq2)\n",
    "        \n",
    "        '''Adding subtopic reconstruction'''\n",
    "            \n",
    "#         self.theta = tf.matmul(self.z0,tf.concat(0,[self.z1, self.z2]))\n",
    "        \n",
    "        self.theta = tf.squeeze(tf.map_fn(lambda i: \n",
    "                                          tf.matmul(tf.expand_dims(tf.gather(self.z0,i),0),\n",
    "                                                    tf.concat(0,[tf.expand_dims(tf.gather(self.z1,i),0),\n",
    "                                                                tf.expand_dims(tf.gather(self.z2,i),0)])),\n",
    "                                                   tf.constant(np.arange(self.batch_size)), dtype=tf.float32))\n",
    "            \n",
    "        self.x_reconstr_mean = \\\n",
    "            self._generator_network(tf.nn.dropout(self.theta, self.keep_prob),\n",
    "                                    self.network_weights[\"weights_gener\"],\n",
    "                                    self.network_weights[\"biases_gener\"])\n",
    "            \n",
    "    def _initialize_weights(self, n_hidden_recog_1, n_hidden_recog_2, \n",
    "                            n_hidden_gener_1,  \n",
    "                            n_input, n_z):\n",
    "        all_weights = dict()\n",
    "        with tf.variable_scope(\"super\"):\n",
    "            all_weights['weights_recog'] = {\n",
    "                'h1': tf.get_variable('h1',[n_input, n_hidden_recog_1]),\n",
    "                'h2': tf.get_variable('h2',[n_hidden_recog_1, n_hidden_recog_2]),\n",
    "                'out_mean': tf.get_variable('out_mean',[n_hidden_recog_2, 2]),\n",
    "                'out_log_sigma': tf.get_variable('out_log_sigma',[n_hidden_recog_2, 2])\n",
    "            }\n",
    "            all_weights['biases_recog'] = {\n",
    "                'b1': tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32)),\n",
    "                'b2': tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32)),\n",
    "                'out_mean': tf.Variable(tf.zeros([2], dtype=tf.float32)),\n",
    "                'out_log_sigma': tf.Variable(tf.zeros([2], dtype=tf.float32))\n",
    "            }\n",
    "        with tf.variable_scope(\"sub01\"):\n",
    "            all_weights['sub_01'] = {\n",
    "                'h1': tf.get_variable('h1',[n_input, n_hidden_recog_1]),\n",
    "                'h2': tf.get_variable('h2',[n_hidden_recog_1, n_hidden_recog_2]),\n",
    "                'out_mean': tf.get_variable('out_mean',[n_hidden_recog_2, n_z]),\n",
    "                'out_log_sigma': tf.get_variable('out_log_sigma',[n_hidden_recog_2, n_z])}\n",
    "            all_weights['biases_sub_01'] = {\n",
    "                'b1': tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32)),\n",
    "                'b2': tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32)),\n",
    "                'out_mean': tf.Variable(tf.zeros([n_z], dtype=tf.float32)),\n",
    "                'out_log_sigma': tf.Variable(tf.zeros([n_z], dtype=tf.float32))\n",
    "            }\n",
    "        with tf.variable_scope(\"sub02\"):\n",
    "            all_weights['sub_02'] = {\n",
    "                'h1': tf.get_variable('h1',[n_input, n_hidden_recog_1]),\n",
    "                'h2': tf.get_variable('h2',[n_hidden_recog_1, n_hidden_recog_2]),\n",
    "                'out_mean': tf.get_variable('out_mean',[n_hidden_recog_2, n_z]),\n",
    "                'out_log_sigma': tf.get_variable('out_log_sigma',[n_hidden_recog_2, n_z])\n",
    "            }\n",
    "            all_weights['biases_sub_02'] = {\n",
    "                'b1': tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32)),\n",
    "                'b2': tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32)),\n",
    "                'out_mean': tf.Variable(tf.zeros([n_z], dtype=tf.float32)),\n",
    "                'out_log_sigma': tf.Variable(tf.zeros([n_z], dtype=tf.float32))\n",
    "            }\n",
    "        with tf.variable_scope(\"gen\"):\n",
    "            all_weights['weights_gener'] = {\n",
    "                'h2': tf.Variable(xavier_init(n_z, n_hidden_gener_1))\n",
    "            }\n",
    "            all_weights['biases_gener'] = {\n",
    "                'b2': tf.Variable(tf.zeros([n_hidden_gener_1], dtype=tf.float32))\n",
    "            }\n",
    "        return all_weights\n",
    "            \n",
    "    def _recognition_network(self, weights, biases):\n",
    "        # Generate probabilistic encoder (recognition network)\n",
    "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.x, weights['h1']), \n",
    "                                           biases['b1'])) \n",
    "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
    "                                           biases['b2'])) \n",
    "        layer_do = tf.nn.dropout(layer_2, self.keep_prob)\n",
    "        \n",
    "        z_mean = tf.contrib.layers.batch_norm(tf.add(tf.matmul(layer_do, weights['out_mean']),\n",
    "                        biases['out_mean']))\n",
    "        z_log_sigma_sq = \\\n",
    "            tf.contrib.layers.batch_norm(tf.add(tf.matmul(layer_do, weights['out_log_sigma']), \n",
    "                   biases['out_log_sigma']))     \n",
    "        \n",
    "        return (z_mean, z_log_sigma_sq)\n",
    "\n",
    "    def _generator_network(self,z, weights, biases):\n",
    "        # Generate probabilistic decoder (decoder network)\n",
    "        self.layer_do_0 = tf.nn.dropout(tf.nn.softmax(tf.contrib.layers.batch_norm(z)), self.keep_prob)\n",
    "#         x_reconstr_mean = tf.add(tf.matmul(self.layer_do_0, \n",
    "#                                            tf.nn.softmax(tf.contrib.layers.batch_norm(weights['h2']))),0.0)\n",
    "        x_reconstr_mean = tf.nn.softmax(tf.contrib.layers.batch_norm(tf.add(\n",
    "                    tf.matmul(self.layer_do_0, weights['h2']),0.0)))\n",
    "    \n",
    "        return x_reconstr_mean\n",
    "\n",
    "            \n",
    "    def _create_loss_optimizer(self):\n",
    "        \n",
    "        self.x_reconstr_mean+=1e-10\n",
    "     \n",
    "        reconstr_loss = \\\n",
    "            -tf.reduce_sum(self.x * tf.log(self.x_reconstr_mean),1)#/tf.reduce_sum(self.x,1) \n",
    "\n",
    "    \n",
    "        latent_loss0 = 0.5*( tf.reduce_sum(tf.div(self.sigma0,self.var2a0),1)+\\\n",
    "        tf.reduce_sum( tf.mul(tf.div((self.mu2a0 - self.z_mean0),self.var2a0),\n",
    "                  (self.mu2a0 - self.z_mean0)),1) - 2 +\\\n",
    "                           tf.reduce_sum(tf.log(self.var2a0),1)  - tf.reduce_sum(self.z_log_sigma_sq0  ,1) ) \n",
    "        \n",
    "        latent_loss1 = 0.5*( tf.reduce_sum(tf.div(self.sigma1,self.var2a1),1)+\\\n",
    "        tf.reduce_sum( tf.mul(tf.div((self.mu2a1 - self.z_mean1),self.var2a1),\n",
    "                  (self.mu2a1 - self.z_mean1)),1) - self.h_dim +\\\n",
    "                           tf.reduce_sum(tf.log(self.var2a1),1)  - tf.reduce_sum(self.z_log_sigma_sq1  ,1) )\n",
    "        \n",
    "        latent_loss2 = 0.5*( tf.reduce_sum(tf.div(self.sigma1,self.var2a2),1)+\\\n",
    "        tf.reduce_sum( tf.mul(tf.div((self.mu2a2 - self.z_mean2),self.var2a2),\n",
    "                  (self.mu2a2 - self.z_mean2)),1) - self.h_dim +\\\n",
    "                           tf.reduce_sum(tf.log(self.var2a2),1)  - tf.reduce_sum(self.z_log_sigma_sq2  ,1) )\n",
    "\n",
    "        \n",
    "        \n",
    "#         latent_loss = -0.5 * tf.reduce_sum(1 + self.z_log_sigma_sq \n",
    "#                                            - tf.square(self.z_mean) \n",
    "#                                            - tf.exp(self.z_log_sigma_sq), 1)\n",
    "\n",
    "\n",
    "        self.cost = tf.reduce_mean(reconstr_loss) \\\n",
    "    + tf.reduce_mean(latent_loss0 + latent_loss1 + latent_loss2) # average over batch\n",
    "        \n",
    "        # Use ADAM optimizer\n",
    "        self.optimizer = \\\n",
    "            tf.train.AdamOptimizer(learning_rate=self.learning_rate,beta1=0.9).minimize(self.cost)\n",
    "        \n",
    "    def partial_fit(self, X):\n",
    "        \"\"\"Train model based on mini-batch of input data.\n",
    "        \n",
    "        Return cost of mini-batch.\n",
    "        \"\"\"\n",
    "#         opt, cost,emb,kld = self.sess.run((self.optimizer, self.cost,self.network_weights['weights_gener']['h2'],\n",
    "#                                           self.kld),feed_dict={self.x: X,self.keep_prob: .9})\n",
    "        opt,cost,emb = self.sess.run((self.optimizer, self.cost,self.network_weights['weights_gener']['h2']\n",
    "                                      ),feed_dict={self.x: X,self.keep_prob: .7})\n",
    "        return cost,emb\n",
    "    \n",
    "    def test(self, X):\n",
    "        \"\"\"Test the model and return the lowerbound on the log-likelihood.\n",
    "        \"\"\"\n",
    "        cost,e1,e2 = self.sess.run((self.cost,tf.nn.softmax(self.z1),tf.nn.softmax(self.z2)),\n",
    "                                        feed_dict={self.x: np.expand_dims(X, axis=0),self.keep_prob: 1.0})\n",
    "        return cost,e1,e2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "cost_plot=[]\n",
    "def train(network_architecture, learning_rate=0.0001,\n",
    "          batch_size=100, training_epochs=10, display_step=5):\n",
    "    vae = VariationalAutoencoder(network_architecture, \n",
    "                                 learning_rate=learning_rate, \n",
    "                                 batch_size=batch_size)\n",
    "    emb=0\n",
    "    avg_kld=0\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(n_samples_tr / batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs = minibatches.next()\n",
    "            # Fit training using batch data\n",
    "            cost,emb = vae.partial_fit(batch_xs)\n",
    "#             print cost\n",
    "#             sys.exit()\n",
    "            # Compute average loss\n",
    "            avg_cost += cost / n_samples_tr * batch_size\n",
    "            \n",
    "            if np.isnan(avg_cost):\n",
    "                print epoch,i,np.sum(batch_xs,1).astype(np.int),batch_xs.shape\n",
    "                return vae,emb\n",
    "#                 sys.exit()\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            cost_plot.append(avg_cost)\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \\\n",
    "                  \"cost=\", \"{:.9f}\".format(avg_cost)\n",
    "\n",
    "    return vae,emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01\n",
      "Epoch: 0001 cost= 11463.845703125\n",
      "Epoch: 0006 cost= 10677.751822917\n",
      "Epoch: 0011 cost= 10271.408463542\n",
      "Epoch: 0016 cost= 10091.700781250\n",
      "Epoch: 0021 cost= 9912.700000000\n",
      "Epoch: 0026 cost= 9669.955468750\n",
      "Epoch: 0031 cost= 9612.331380208\n",
      "Epoch: 0036 cost= 9567.632812500\n",
      "Epoch: 0041 cost= 9393.428515625\n",
      "Epoch: 0046 cost= 9489.477734375\n",
      "Epoch: 0051 cost= 9377.751692708\n",
      "Epoch: 0056 cost= 9276.521614583\n",
      "Epoch: 0061 cost= 9295.638671875\n",
      "Epoch: 0066 cost= 9249.142968750\n",
      "Epoch: 0071 cost= 9256.298567708\n",
      "Epoch: 0076 cost= 9391.197526042\n",
      "Epoch: 0081 cost= 9342.828385417\n",
      "Epoch: 0086 cost= 9320.726822917\n",
      "Epoch: 0091 cost= 9215.494531250\n",
      "Epoch: 0096 cost= 9224.305859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AssertionError: AssertionError(\"Nesting violated for default stack of <type 'weakref'> objects\",) in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0xf7343d0>> ignored\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nChange log: decoder has no dropout or additional tranformations\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "network_architecture = \\\n",
    "    dict(n_hidden_recog_1=100, # 1st layer encoder neurons\n",
    "         n_hidden_recog_2=100, # 2nd layer encoder neurons\n",
    "         n_hidden_gener_1=docs_tr.shape[1], # 1st layer decoder neurons\n",
    "         n_input=docs_tr.shape[1], # MNIST data input (img shape: 28*28)\n",
    "         n_z=10)  # dimensionality of latent space\n",
    "\n",
    "batch_size=200\n",
    "learning_rate=0.01\n",
    "def create_minibatch(data):\n",
    "    rng = np.random.RandomState(10)\n",
    "    \n",
    "    while True:\n",
    "        # Return random data samples of a size 'minibatch_size' at each iteration\n",
    "        ixs = rng.randint(data.shape[0], size=batch_size)\n",
    "        yield data[ixs]\n",
    "        \n",
    "minibatches = create_minibatch(docs_tr.astype('float32'))\n",
    "vae,emb = train(network_architecture, training_epochs=100,batch_size=batch_size,learning_rate=learning_rate)\n",
    "# plt.plot(cost_plot)\n",
    "\n",
    "\n",
    "'''\n",
    "Change log: decoder has no dropout or additional tranformations\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker hmm classifier viterbi fom talker phonetic speech male female\n",
      "rat hippocampal ca1 mcnaughton keefe head hippocampus firing subicular food\n",
      "theorem lemma proof bound maass vapnik margin sgn pac corollary\n",
      "firing bulb membrane olfactory odor spiking granule physiol conductance spike\n",
      "voltage circuit transistor silicon mead capacitor vlsi voltages cmos fabricated\n",
      "hyperparameter amari likelihood variational density cichocki posterior deconvolution bayesian mixture\n",
      "policy singh tsitsiklis mdp agent bertsekas pomdp discounted precup policies\n",
      "reinforcement policy watkin barto controller kawato policies swing routing movement\n",
      "wattle rap clones parsing parsec phrase chunk tomita stack parser\n",
      "reflectance occlusion tangent bregler manifold parietal illumination images facial occluded\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(beta, feature_names, n_top_words=10):\n",
    "    for i in range(len(beta)):\n",
    "        print(\" \".join([feature_names[j]\n",
    "            for j in beta[i].argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "print_top_words(emb, \"\".join(vocab).split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cost=[]\n",
    "e1=[]\n",
    "e2=[]\n",
    "idx=0\n",
    "for doc in docs_te:\n",
    "    idx+=1\n",
    "    doc = doc.astype('float32')\n",
    "    n_d = np.sum(doc)\n",
    "    c,a,b=vae.test(doc)\n",
    "    cost.append(c/n_d)\n",
    "    e1.append(a)\n",
    "    e2.append(b)\n",
    "    if idx%1000==0:\n",
    "        print idx\n",
    "    \n",
    "# print (np.exp(np.mean(np.array(cost))))\n",
    "# print np.mean(e1,0)\n",
    "# print np.mean(e2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('nips.txt','w') as o:\n",
    "\n",
    "    for doc in docs_tr:\n",
    "        doc_o=[]\n",
    "        j=-1\n",
    "        for wc in doc:\n",
    "            j+=1\n",
    "            for i in range(int(wc)):\n",
    "                doc_o.append(vocab[j].strip())\n",
    "        o.write(\" \".join(doc_o)+'\\n')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(cost,open('cost','w'))\n",
    "pickle.dump(e1,open('e1','w'))\n",
    "pickle.dump(e2,open('e2','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
