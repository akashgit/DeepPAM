{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/group/cup/data1/akash_itm/IPC_MAP/lib/python2.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools,time\n",
    "import sys, os\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 12419) (12419,)\n"
     ]
    }
   ],
   "source": [
    "dataset_tr = '/afs/inf.ed.ac.uk/group/cup/data1/akash_itm/IPC_MAP/NTP/process_UCI/M_nips.full_docs.mat'\n",
    "data_tr = sp.io.loadmat(dataset_tr)['M'].T.toarray()\n",
    "\n",
    "vocab_ = '/afs/inf.ed.ac.uk/group/cup/data1/akash_itm/IPC_MAP/NTP/process_UCI/vocab.nips.txt'\n",
    "vocab_ = open(vocab_,'r').readlines()\n",
    "vocab=np.array([word for word in vocab_])\n",
    "\n",
    "print data_tr.shape, vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_size=len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples_tr = data_tr.shape[0]\n",
    "docs_tr = data_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def xavier_init(fan_in, fan_out, constant=1): \n",
    "    \"\"\" Xavier initialization of network weights\"\"\"\n",
    "    # https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "    low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "    high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out), \n",
    "                             minval=low, maxval=high, \n",
    "                             dtype=tf.float32)\n",
    "def log_dir_init(fan_in, fan_out,topics=50): \n",
    "    return tf.log((1.0/topics)*tf.ones([fan_in, fan_out]))\n",
    "\n",
    "tf.reset_default_graph()\n",
    "class VariationalAutoencoder(object):\n",
    "    \"\"\" \n",
    "    See \"Auto-Encoding Variational Bayes\" by Kingma and Welling for more details.\n",
    "    \"\"\"\n",
    "    def __init__(self, network_architecture, transfer_fct=tf.nn.softplus, \n",
    "                 learning_rate=0.001, batch_size=100):\n",
    "        self.network_architecture = network_architecture\n",
    "        self.transfer_fct = transfer_fct\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        print 'Learning Rate:', self.learning_rate\n",
    "        \n",
    "        # tf Graph input\n",
    "        self.x = tf.placeholder(tf.float32, [None, network_architecture[\"n_input\"]])\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        self.is_training = tf.placeholder(tf.bool, [], name='is_training')\n",
    "        self.samples = tf.placeholder(tf.int32)\n",
    "        \n",
    "        self.h_dim = float(network_architecture[\"n_z\"])\n",
    "        self.a0 = np.ones((1 , 2)).astype(np.float32)/2\n",
    "        self.a1 = np.ones((1 , self.h_dim)).astype(np.float32)/self.h_dim\n",
    "        self.a2 = np.ones((1 , self.h_dim)).astype(np.float32)/self.h_dim\n",
    "        \n",
    "        self.mu2a0 = tf.constant((np.log(self.a0).T-np.mean(np.log(self.a0),1)).T)\n",
    "        self.var2a0 = tf.constant(  ( ( (1.0/self.a0)*( 1 - (2.0/2.0) ) ).T +\n",
    "                                ( 1.0/(self.h_dim*self.h_dim) )*np.sum(1.0/self.a0,1) ).T  )\n",
    "        \n",
    "        self.mu2a1 = tf.constant((np.log(self.a1).T-np.mean(np.log(self.a1),1)).T)\n",
    "        self.var2a1 = tf.constant(  ( ( (1.0/self.a1)*( 1 - (2.0/self.h_dim) ) ).T +\n",
    "                                ( 1.0/(self.h_dim*self.h_dim) )*np.sum(1.0/self.a1,1) ).T  )\n",
    "        \n",
    "        self.mu2a2 = tf.constant((np.log(self.a2).T-np.mean(np.log(self.a2),1)).T)\n",
    "        self.var2a2 = tf.constant(  ( ( (1.0/self.a2)*( 1 - (2.0/self.h_dim) ) ).T +\n",
    "                                ( 1.0/(self.h_dim*self.h_dim) )*np.sum(1.0/self.a2,1) ).T  )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Create autoencoder network\n",
    "        self._create_network()\n",
    "        # Define loss function based variational upper-bound and \n",
    "        # corresponding optimizer\n",
    "        self._create_loss_optimizer()\n",
    "        \n",
    "        # Initializing the tensor flow variables\n",
    "        init = tf.initialize_all_variables()\n",
    "\n",
    "        # Launch the session\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(init)\n",
    "    \n",
    "    def _create_network(self):\n",
    "        n_z = self.network_architecture[\"n_z\"]\n",
    "        # Initialize autoencode network weights and biases\n",
    "        self.network_weights = self._initialize_weights(**self.network_architecture)\n",
    "        '''Adding 2 subtopics and 1 super topic.'''\n",
    "        self.z_mean0,self.z_log_sigma_sq0 = \\\n",
    "            self._recognition_network(self.network_weights[\"weights_recog\"], \n",
    "                                      self.network_weights[\"biases_recog\"])\n",
    "            \n",
    "        self.z_mean_,self.z_log_sigma_sq_ = \\\n",
    "            self._recognition_network(self.network_weights[\"sub_01\"], \n",
    "                                      self.network_weights[\"biases_sub_01\"])\n",
    "            \n",
    "        self.z_mean1=self.z_mean_[:,:2]\n",
    "        self.z_mean2=self.z_mean_[:,2:]\n",
    "        self.z_log_sigma_sq1=self.z_log_sigma_sq_[:,:2]\n",
    "        self.z_log_sigma_sq2=self.z_log_sigma_sq_[:,2:]\n",
    "        \n",
    "            \n",
    "        self.z_mean__,self.z_log_sigma_sq__ = \\\n",
    "            self._recognition_network(self.network_weights[\"sub_02\"], \n",
    "                                      self.network_weights[\"biases_sub_02\"])\n",
    "            \n",
    "        self.z_mean3=self.z_mean__[:,:n_z]\n",
    "        self.z_mean4=self.z_mean__[:,n_z:]\n",
    "        self.z_log_sigma_sq3=self.z_log_sigma_sq__[:,:n_z]\n",
    "        self.z_log_sigma_sq4=self.z_log_sigma_sq__[:,n_z:]\n",
    "        \n",
    "\n",
    "\n",
    "        # Draw one sample z from Gaussian distribution\n",
    "        '''Adding another noise variable for super toipc: hard coding atm'''\n",
    "        eps0 = tf.random_normal((self.samples, 2), 0, 1, \n",
    "                               dtype=tf.float32)\n",
    "        \n",
    "        eps1 = tf.random_normal((self.samples, 2), 0, 1, \n",
    "                               dtype=tf.float32)\n",
    "        eps2 = tf.random_normal((self.samples, n_z), 0, 1, \n",
    "                               dtype=tf.float32)\n",
    "        '''Adding RT for subtopics'''\n",
    "        self.z0 = tf.nn.softmax(tf.contrib.layers.batch_norm(tf.add(self.z_mean0, \n",
    "                        tf.mul(tf.sqrt(tf.exp(self.z_log_sigma_sq0)), eps0))))\n",
    "        self.z0 = tf.nn.softmax(tf.contrib.layers.batch_norm(self.z0,\n",
    "                                                             is_training=self.is_training,updates_collections=None))\n",
    "        \n",
    "        self.sigma0 = tf.exp(self.z_log_sigma_sq0)\n",
    "        \n",
    "        self.z1 = tf.add(self.z_mean1, \n",
    "                        tf.mul(tf.sqrt(tf.exp(self.z_log_sigma_sq1)), eps1))\n",
    "        self.z1 = tf.nn.softmax(tf.contrib.layers.batch_norm(self.z1,\n",
    "                                                            is_training=self.is_training,updates_collections=None))\n",
    "        \n",
    "        self.sigma1 = tf.exp(self.z_log_sigma_sq1)\n",
    "        \n",
    "        self.z2 = tf.add(self.z_mean2, \n",
    "                        tf.mul(tf.sqrt(tf.exp(self.z_log_sigma_sq2)), eps1))\n",
    "        self.z2 = tf.nn.softmax(tf.contrib.layers.batch_norm(self.z2,\n",
    "                                                            is_training=self.is_training,updates_collections=None))\n",
    "        \n",
    "        self.sigma2 = tf.exp(self.z_log_sigma_sq2)\n",
    "        \n",
    "        self.z3 = tf.add(self.z_mean3, \n",
    "                        tf.mul(tf.sqrt(tf.exp(self.z_log_sigma_sq3)), eps2))\n",
    "        self.z3 = tf.nn.softmax(tf.contrib.layers.batch_norm(self.z3,\n",
    "                                                            is_training=self.is_training,updates_collections=None))\n",
    "        \n",
    "        self.sigma3 = tf.exp(self.z_log_sigma_sq3)\n",
    "        \n",
    "        self.z4 = tf.add(self.z_mean4, \n",
    "                        tf.mul(tf.sqrt(tf.exp(self.z_log_sigma_sq4)), eps2))\n",
    "        self.z4 = tf.nn.softmax(tf.contrib.layers.batch_norm(self.z4,\n",
    "                                                            is_training=self.is_training,updates_collections=None))\n",
    "        \n",
    "        self.sigma4 = tf.exp(self.z_log_sigma_sq4)\n",
    "        \n",
    "        '''Adding subtopic reconstruction'''\n",
    "            \n",
    "#         self.theta = tf.matmul(self.z0,tf.concat(0,[self.z1, self.z2]))\n",
    "        \n",
    "        self.theta_ = tf.squeeze(tf.map_fn(lambda i: \n",
    "                                          tf.matmul(tf.expand_dims(tf.gather(self.z0,i),0),\n",
    "                                                    tf.concat(0,[tf.expand_dims(tf.gather(self.z1,i),0),\n",
    "                                                                tf.expand_dims(tf.gather(self.z2,i),0)])),\n",
    "                                                   tf.constant(np.arange(self.batch_size)), dtype=tf.float32))\n",
    "        \n",
    "        self.theta = tf.squeeze(tf.map_fn(lambda i: \n",
    "                                          tf.matmul(tf.expand_dims(tf.gather(self.theta_,i),0),\n",
    "                                                    tf.concat(0,[tf.expand_dims(tf.gather(self.z3,i),0),\n",
    "                                                                tf.expand_dims(tf.gather(self.z4,i),0)])),\n",
    "                                                   tf.constant(np.arange(self.batch_size)), dtype=tf.float32))\n",
    "            \n",
    "        self.x_reconstr_mean = \\\n",
    "            self._generator_network(tf.nn.dropout(self.theta, self.keep_prob),\n",
    "                                    self.network_weights[\"weights_gener\"],\n",
    "                                    self.network_weights[\"biases_gener\"])\n",
    "            \n",
    "    def _initialize_weights(self, n_hidden_recog_1, n_hidden_recog_2, \n",
    "                            n_hidden_gener_1,  \n",
    "                            n_input, n_z):\n",
    "        all_weights = dict()\n",
    "        with tf.variable_scope(\"super\"):\n",
    "            all_weights['weights_recog'] = {\n",
    "                'h1': tf.get_variable('h1',[n_input, n_hidden_recog_1]),\n",
    "                'h2': tf.get_variable('h2',[n_hidden_recog_1, n_hidden_recog_2]),\n",
    "                'out_mean': tf.get_variable('out_mean',[n_hidden_recog_2, 2]),\n",
    "                'out_log_sigma': tf.get_variable('out_log_sigma',[n_hidden_recog_2, 2])\n",
    "            }\n",
    "            all_weights['biases_recog'] = {\n",
    "                'b1': tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32)),\n",
    "                'b2': tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32)),\n",
    "                'out_mean': tf.Variable(tf.zeros([2], dtype=tf.float32)),\n",
    "                'out_log_sigma': tf.Variable(tf.zeros([2], dtype=tf.float32))\n",
    "            }\n",
    "        with tf.variable_scope(\"sub01\"):\n",
    "            all_weights['sub_01'] = {\n",
    "                'h1': tf.get_variable('h1',[n_input, n_hidden_recog_1]),\n",
    "                'h2': tf.get_variable('h2',[n_hidden_recog_1, n_hidden_recog_2]),\n",
    "                'out_mean': tf.get_variable('out_mean',[n_hidden_recog_2, 2*2]),\n",
    "                'out_log_sigma': tf.get_variable('out_log_sigma',[n_hidden_recog_2, 2*2])}\n",
    "            all_weights['biases_sub_01'] = {\n",
    "                'b1': tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32)),\n",
    "                'b2': tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32)),\n",
    "                'out_mean': tf.Variable(tf.zeros([2*2], dtype=tf.float32)),\n",
    "                'out_log_sigma': tf.Variable(tf.zeros([2*2], dtype=tf.float32))\n",
    "            }\n",
    "        with tf.variable_scope(\"sub02\"):\n",
    "            all_weights['sub_02'] = {\n",
    "                'h1': tf.get_variable('h1',[n_input, n_hidden_recog_1]),\n",
    "                'h2': tf.get_variable('h2',[n_hidden_recog_1, n_hidden_recog_2]),\n",
    "                'out_mean': tf.get_variable('out_mean',[n_hidden_recog_2, n_z*2]),\n",
    "                'out_log_sigma': tf.get_variable('out_log_sigma',[n_hidden_recog_2, n_z*2])\n",
    "            }\n",
    "            all_weights['biases_sub_02'] = {\n",
    "                'b1': tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32)),\n",
    "                'b2': tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32)),\n",
    "                'out_mean': tf.Variable(tf.zeros([n_z*2], dtype=tf.float32)),\n",
    "                'out_log_sigma': tf.Variable(tf.zeros([n_z*2], dtype=tf.float32))\n",
    "            }\n",
    "        with tf.variable_scope(\"gen\"):\n",
    "            all_weights['weights_gener'] = {\n",
    "                'h2': tf.Variable(xavier_init(n_z, n_hidden_gener_1))\n",
    "            }\n",
    "            all_weights['biases_gener'] = {\n",
    "                'b2': tf.Variable(tf.zeros([n_hidden_gener_1], dtype=tf.float32))\n",
    "            }\n",
    "        return all_weights\n",
    "            \n",
    "    def _recognition_network(self, weights, biases):\n",
    "        # Generate probabilistic encoder (recognition network)\n",
    "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.x, weights['h1']), \n",
    "                                           biases['b1'])) \n",
    "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
    "                                           biases['b2'])) \n",
    "        layer_do = tf.nn.dropout(layer_2, self.keep_prob)\n",
    "        \n",
    "        z_mean = tf.contrib.layers.batch_norm(tf.add(tf.matmul(layer_do, weights['out_mean']),\n",
    "                        biases['out_mean']),is_training=self.is_training,updates_collections=None)\n",
    "        z_log_sigma_sq = \\\n",
    "            tf.contrib.layers.batch_norm(tf.add(tf.matmul(layer_do, weights['out_log_sigma']), \n",
    "                   biases['out_log_sigma']),is_training=self.is_training,updates_collections=None)     \n",
    "        \n",
    "        return (z_mean, z_log_sigma_sq)\n",
    "\n",
    "    def _generator_network(self,z, weights, biases):\n",
    "        # Generate probabilistic decoder (decoder network)\n",
    "#         self.layer_do_0 = tf.nn.dropout(tf.nn.softmax(tf.contrib.layers.batch_norm(z)), self.keep_prob)\n",
    "        self.layer_do_0 =z\n",
    "        x_reconstr_mean = tf.add(tf.matmul(self.layer_do_0, \n",
    "                                           tf.nn.softmax(tf.contrib.layers.batch_norm(weights['h2'],\\\n",
    "is_training=self.is_training,updates_collections=None))),0.0)\n",
    "#         x_reconstr_mean = tf.nn.softmax(tf.contrib.layers.batch_norm(tf.add(\n",
    "#                     tf.matmul(self.layer_do_0, weights['h2']),0.0)))\n",
    "    \n",
    "        return x_reconstr_mean\n",
    "\n",
    "            \n",
    "    def _create_loss_optimizer(self):\n",
    "        \n",
    "        self.x_reconstr_mean+=1e-10\n",
    "     \n",
    "        reconstr_loss = \\\n",
    "            -tf.reduce_sum(self.x * tf.log(self.x_reconstr_mean),1)#/tf.reduce_sum(self.x,1) \n",
    "\n",
    "    \n",
    "        latent_loss0 = 0.5*( tf.reduce_sum(tf.div(self.sigma0,self.var2a0),1)+\\\n",
    "        tf.reduce_sum( tf.mul(tf.div((self.mu2a0 - self.z_mean0),self.var2a0),\n",
    "                  (self.mu2a0 - self.z_mean0)),1) - 2 +\\\n",
    "                           tf.reduce_sum(tf.log(self.var2a0),1)  - tf.reduce_sum(self.z_log_sigma_sq0  ,1) ) \n",
    "        \n",
    "        latent_loss1 = 0.5*( tf.reduce_sum(tf.div(self.sigma1,self.var2a0),1)+\\\n",
    "        tf.reduce_sum( tf.mul(tf.div((self.mu2a0 - self.z_mean1),self.var2a0),\n",
    "                  (self.mu2a0 - self.z_mean1)),1) - 2 +\\\n",
    "                           tf.reduce_sum(tf.log(self.var2a0),1)  - tf.reduce_sum(self.z_log_sigma_sq1  ,1) ) \n",
    "        \n",
    "        latent_loss2 = 0.5*( tf.reduce_sum(tf.div(self.sigma2,self.var2a0),1)+\\\n",
    "        tf.reduce_sum( tf.mul(tf.div((self.mu2a0 - self.z_mean2),self.var2a0),\n",
    "                  (self.mu2a0 - self.z_mean2)),1) - 2 +\\\n",
    "                           tf.reduce_sum(tf.log(self.var2a0),1)  - tf.reduce_sum(self.z_log_sigma_sq2  ,1) ) \n",
    "        \n",
    "        latent_loss3 = 0.5*( tf.reduce_sum(tf.div(self.sigma3,self.var2a1),1)+\\\n",
    "        tf.reduce_sum( tf.mul(tf.div((self.mu2a1 - self.z_mean3),self.var2a1),\n",
    "                  (self.mu2a1 - self.z_mean3)),1) - self.h_dim +\\\n",
    "                           tf.reduce_sum(tf.log(self.var2a1),1)  - tf.reduce_sum(self.z_log_sigma_sq3  ,1) )\n",
    "        \n",
    "        latent_loss4 = 0.5*( tf.reduce_sum(tf.div(self.sigma4,self.var2a2),1)+\\\n",
    "        tf.reduce_sum( tf.mul(tf.div((self.mu2a2 - self.z_mean4),self.var2a2),\n",
    "                  (self.mu2a2 - self.z_mean4)),1) - self.h_dim +\\\n",
    "                           tf.reduce_sum(tf.log(self.var2a2),1)  - tf.reduce_sum(self.z_log_sigma_sq4  ,1) )\n",
    "\n",
    "        \n",
    "        \n",
    "#         latent_loss = -0.5 * tf.reduce_sum(1 + self.z_log_sigma_sq \n",
    "#                                            - tf.square(self.z_mean) \n",
    "#                                            - tf.exp(self.z_log_sigma_sq), 1)\n",
    "\n",
    "\n",
    "        self.cost = tf.reduce_mean(reconstr_loss) \\\n",
    "    + tf.reduce_mean(latent_loss0 + latent_loss1 + latent_loss2 + latent_loss3 + latent_loss4) # average over batch\n",
    "        \n",
    "        # Use ADAM optimizer\n",
    "        self.optimizer = \\\n",
    "            tf.train.AdamOptimizer(learning_rate=self.learning_rate,beta1=0.9).minimize(self.cost)\n",
    "        \n",
    "    def partial_fit(self, X):\n",
    "        \"\"\"Train model based on mini-batch of input data.\n",
    "        \n",
    "        Return cost of mini-batch.\n",
    "        \"\"\"\n",
    "#         opt, cost,emb,kld = self.sess.run((self.optimizer, self.cost,self.network_weights['weights_gener']['h2'],\n",
    "#                                           self.kld),feed_dict={self.x: X,self.keep_prob: .9})\n",
    "        opt,cost,emb = self.sess.run((self.optimizer, self.cost,self.network_weights['weights_gener']['h2']\n",
    "                                      ),feed_dict={self.x: X,self.keep_prob: .7,self.is_training:True,self.samples:1})\n",
    "        return cost,emb\n",
    "    \n",
    "    def test(self, X):\n",
    "        \"\"\"Test the model and return the lowerbound on the log-likelihood.\n",
    "        \"\"\"\n",
    "        cost,e1,e2,e3,e4 = self.sess.run((self.cost,self.z1,self.z2,self.z3,self.z4),\n",
    "                                        feed_dict={self.x: np.expand_dims(X, axis=0),self.keep_prob: 1.0,\n",
    "                                                   self.is_training:False,self.samples:1})\n",
    "        return cost,e1,e2,e3,e4\n",
    "    \n",
    "    def batchtest(self, X):\n",
    "        \"\"\"Test the model and return the lowerbound on the log-likelihood.\n",
    "        \"\"\"\n",
    "        cost,e1,e2,e3,e4 = self.sess.run((self.cost,self.z1,self.z2,self.z3,self.z4),\n",
    "                                        feed_dict={self.x: X,self.keep_prob: 1.0,\n",
    "                                                   self.is_training:False,self.samples:1})\n",
    "        return cost,e1,e2,e3,e4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "cost_plot=[]\n",
    "test_batch = create_minibatch(docs_tr.astype('float32'))\n",
    "def train(network_architecture, learning_rate=0.0001,\n",
    "          batch_size=100, training_epochs=10, display_step=5):\n",
    "    vae = VariationalAutoencoder(network_architecture, \n",
    "                                 learning_rate=learning_rate, \n",
    "                                 batch_size=batch_size)\n",
    "    emb=0\n",
    "    avg_kld=0\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        avg_kld=0.\n",
    "        total_batch = int(n_samples_tr / batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs = minibatches.next()\n",
    "            # Fit training using batch data\n",
    "            cost,emb = vae.partial_fit(batch_xs)\n",
    "#             c,a,b,k,l = vae.batchtest(batch_xs)\n",
    "\n",
    "#             sys.exit()\n",
    "            # Compute average loss\n",
    "            avg_cost += cost / n_samples_tr * batch_size\n",
    "#             avg_kld += c / n_samples_tr * batch_size\n",
    "            \n",
    "            if np.isnan(avg_cost):\n",
    "                print epoch,i,np.sum(batch_xs,1).astype(np.int),batch_xs.shape\n",
    "                return vae,emb\n",
    "#                 sys.exit()\n",
    "        \n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            cost_plot.append(avg_cost)\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \\\n",
    "                  \"cost=\", \"{:.9f}\".format(avg_cost)\n",
    "#             print 'avg_cost = ',avg_kld\n",
    "            test_c=0.\n",
    "#             for i in range(int(n_samples_te / batch_size)):\n",
    "#                 c,a,b,k,l = vae.batchtest(test_batch.next())\n",
    "#                 test_c+=c / n_samples_te * batch_size\n",
    "#             print 'avg_test_cost = ',test_c\n",
    "    return vae,emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/group/cup/data1/akash_itm/IPC_MAP/lib/python2.7/site-packages/numpy/core/numeric.py:190: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  a = empty(shape, dtype, order)\n",
      "/afs/inf.ed.ac.uk/group/cup/data1/akash_itm/IPC_MAP/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py:90: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.005\n",
      "Epoch: 0001 cost= 15710.180338542\n",
      "Epoch: 0006 cost= 13740.960286458\n",
      "Epoch: 0011 cost= 12995.959375000\n",
      "Epoch: 0016 cost= 12722.149348958\n",
      "Epoch: 0021 cost= 12525.194270833\n",
      "Epoch: 0026 cost= 12197.385156250\n",
      "Epoch: 0031 cost= 12114.079687500\n",
      "Epoch: 0036 cost= 12047.057812500\n",
      "Epoch: 0041 cost= 11832.433203125\n",
      "Epoch: 0046 cost= 11864.229296875\n",
      "Epoch: 0051 cost= 11742.731640625\n",
      "Epoch: 0056 cost= 11573.179687500\n",
      "Epoch: 0061 cost= 11535.747395833\n",
      "Epoch: 0066 cost= 11468.102473958\n",
      "Epoch: 0071 cost= 11439.629427083\n",
      "Epoch: 0076 cost= 11530.482291667\n",
      "Epoch: 0081 cost= 11448.210677083\n",
      "Epoch: 0086 cost= 11378.112760417\n",
      "Epoch: 0091 cost= 11268.363411458\n",
      "Epoch: 0096 cost= 11238.934114583\n",
      "Epoch: 0101 cost= 11306.197786458\n",
      "Epoch: 0106 cost= 11344.061979167\n",
      "Epoch: 0111 cost= 11207.384244792\n",
      "Epoch: 0116 cost= 11243.599479167\n",
      "Epoch: 0121 cost= 11137.137109375\n",
      "Epoch: 0126 cost= 11148.302213542\n",
      "Epoch: 0131 cost= 11176.147526042\n",
      "Epoch: 0136 cost= 11119.773958333\n",
      "Epoch: 0141 cost= 11114.797395833\n",
      "Epoch: 0146 cost= 11138.675390625\n",
      "Epoch: 0151 cost= 11080.052734375\n",
      "Epoch: 0156 cost= 11084.918098958\n",
      "Epoch: 0161 cost= 11013.626432292\n",
      "Epoch: 0166 cost= 11176.770833333\n",
      "Epoch: 0171 cost= 11047.556250000\n",
      "Epoch: 0176 cost= 11025.017447917\n",
      "Epoch: 0181 cost= 11015.483072917\n",
      "Epoch: 0186 cost= 10984.385937500\n",
      "Epoch: 0191 cost= 11033.006901042\n",
      "Epoch: 0196 cost= 11011.557812500\n",
      "Epoch: 0201 cost= 11041.112369792\n",
      "Epoch: 0206 cost= 11042.007031250\n",
      "Epoch: 0211 cost= 10934.354557292\n",
      "Epoch: 0216 cost= 10954.938281250\n",
      "Epoch: 0221 cost= 10963.135156250\n",
      "Epoch: 0226 cost= 11037.519791667\n",
      "Epoch: 0231 cost= 10872.515494792\n",
      "Epoch: 0236 cost= 10919.586458333\n",
      "Epoch: 0241 cost= 10890.285807292\n",
      "Epoch: 0246 cost= 10891.668229167\n",
      "Epoch: 0251 cost= 10961.912760417\n",
      "Epoch: 0256 cost= 10976.111458333\n",
      "Epoch: 0261 cost= 10925.305208333\n",
      "Epoch: 0266 cost= 10896.766145833\n",
      "Epoch: 0271 cost= 10995.710026042\n",
      "Epoch: 0276 cost= 10993.881640625\n",
      "Epoch: 0281 cost= 10923.465364583\n",
      "Epoch: 0286 cost= 10962.477473958\n",
      "Epoch: 0291 cost= 10959.490755208\n",
      "Epoch: 0296 cost= 10805.427864583\n",
      "Epoch: 0301 cost= 10968.100000000\n",
      "Epoch: 0306 cost= 10978.606250000\n",
      "Epoch: 0311 cost= 10932.649869792\n",
      "Epoch: 0316 cost= 10898.189583333\n",
      "Epoch: 0321 cost= 10803.205598958\n",
      "Epoch: 0326 cost= 10886.685026042\n",
      "Epoch: 0331 cost= 10938.275651042\n",
      "Epoch: 0336 cost= 10881.762760417\n",
      "Epoch: 0341 cost= 10889.704817708\n",
      "Epoch: 0346 cost= 10826.471484375\n",
      "Epoch: 0351 cost= 10873.862500000\n",
      "Epoch: 0356 cost= 10819.659244792\n",
      "Epoch: 0361 cost= 10928.244661458\n",
      "Epoch: 0366 cost= 10815.399348958\n",
      "Epoch: 0371 cost= 10732.575130208\n",
      "Epoch: 0376 cost= 10800.366927083\n",
      "Epoch: 0381 cost= 10769.499869792\n",
      "Epoch: 0386 cost= 10789.551432292\n",
      "Epoch: 0391 cost= 10796.582161458\n",
      "Epoch: 0396 cost= 10765.066927083\n",
      "Epoch: 0401 cost= 10863.102343750\n",
      "Epoch: 0406 cost= 10810.993359375\n",
      "Epoch: 0411 cost= 10837.533203125\n",
      "Epoch: 0416 cost= 10814.730989583\n",
      "Epoch: 0421 cost= 10854.977083333\n",
      "Epoch: 0426 cost= 10851.012760417\n",
      "Epoch: 0431 cost= 10826.300260417\n",
      "Epoch: 0436 cost= 10783.981119792\n",
      "Epoch: 0441 cost= 10804.457161458\n",
      "Epoch: 0446 cost= 10803.853906250\n",
      "Epoch: 0451 cost= 10743.185546875\n",
      "Epoch: 0456 cost= 10744.205598958\n",
      "Epoch: 0461 cost= 10712.370833333\n",
      "Epoch: 0466 cost= 10763.600781250\n",
      "Epoch: 0471 cost= 10839.974218750\n",
      "Epoch: 0476 cost= 10714.413802083\n",
      "Epoch: 0481 cost= 10788.142057292\n",
      "Epoch: 0486 cost= 10762.758072917\n",
      "Epoch: 0491 cost= 10736.487630208\n",
      "Epoch: 0496 cost= 10738.037630208\n",
      "it took 1312.02797103 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nChange log: decoder has no dropout or additional tranformations\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "network_architecture = \\\n",
    "    dict(n_hidden_recog_1=400, # 1st layer encoder neurons\n",
    "         n_hidden_recog_2=400, # 2nd layer encoder neurons\n",
    "         n_hidden_gener_1=docs_tr.shape[1], # 1st layer decoder neurons\n",
    "         n_input=docs_tr.shape[1], # MNIST data input (img shape: 28*28)\n",
    "         n_z=50)  # dimensionality of latent space\n",
    "\n",
    "batch_size=200\n",
    "learning_rate=0.005\n",
    "def create_minibatch(data):\n",
    "    rng = np.random.RandomState(10)\n",
    "    \n",
    "    while True:\n",
    "        # Return random data samples of a size 'minibatch_size' at each iteration\n",
    "        ixs = rng.randint(data.shape[0], size=batch_size)\n",
    "        yield data[ixs]\n",
    "start=time.time()\n",
    "minibatches = create_minibatch(docs_tr.astype('float32'))\n",
    "vae,emb = train(network_architecture, training_epochs=500,batch_size=batch_size,learning_rate=learning_rate)\n",
    "# plt.plot(cost_plot)\n",
    "print 'it took',str(time.time()-start),'seconds'\n",
    "\n",
    "\n",
    "'''\n",
    "Change log: decoder has no dropout or additional tranformations\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conscious item psychological psychology consciousness\n",
      "codeword code decoded ziv encoding\n",
      "policy reinforcement tsitsiklis transition action\n",
      "nectar bees encounter montague foraging\n",
      "frequency cell signal response zee\n",
      "rfwr expertise pert blending expert\n",
      "parcor speech jaw tongue koike\n",
      "distribution matrix gaussian mean non\n",
      "location field image motion visual\n",
      "layer input unit paperr network\n",
      "differ mainly common tend greater\n",
      "hassibi order error thorbergsson gradient\n",
      "layer hinton network input interactively\n",
      "neuron synaptic synapses potential spike\n",
      "linesearch kushner annealing algorithm ribiere\n",
      "intrapersonal scientist perona feret rois\n",
      "gopal sdh program marko nlist\n",
      "activity field cell visual neuron\n",
      "string recurrent languages automata subgraph\n",
      "john kevin pierre scott alan\n",
      "graph clustering cluster gurewitz musk\n",
      "positive examples auer class algorithm\n",
      "hopf gutfreund hopfield dynamic phase\n",
      "pacemaker beer chiel heterogeneity raven\n",
      "bound theorem positive function cornell\n",
      "control dynamic controller robot learning\n",
      "subexpression promoter backer shavlik towell\n",
      "regulatory gene mrna kinase rescorla\n",
      "ebd diaconis ppr esp farmer\n",
      "analog chip processor circuit bit\n",
      "analog circuit vlsi chip current\n",
      "image images object scale invariant\n",
      "cai mai parallelizable bernhard tai\n",
      "movement motor foveation platform obstacles\n",
      "model likelihood probability structure distribution\n",
      "recognition speech word system context\n",
      "block interpreter fortran sesame processor\n",
      "data model distribution regression diabetes\n",
      "played alterna issues noteworthy investigating\n",
      "annealing graph point optimization constraint\n",
      "string languages syntactic clause phrase\n",
      "signal frequency filter channel unnikrishnan\n",
      "threshold maass computational graz igi\n",
      "clinical cardiac click makeig clin\n",
      "prediction stb artmap mtb prices\n",
      "training performance test set mif\n",
      "policy decision reinforcement wait reward\n",
      "overlap metastable susceptibility temperatures spin\n",
      "ies loosely unique clear looking\n",
      "component wavelet matrix analysis source\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(beta, feature_names, n_top_words=5):\n",
    "    for i in range(len(beta)):\n",
    "        print(\" \".join([feature_names[j]\n",
    "            for j in beta[i].argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "print_top_words(emb, \"\".join(vocab).split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "3708.81\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cost=[]\n",
    "e1=[]\n",
    "e2=[]\n",
    "e3=[]\n",
    "e4=[]\n",
    "idx=0\n",
    "for doc in docs_te:\n",
    "    idx+=1\n",
    "    doc = doc.astype('float32')\n",
    "    n_d = np.sum(doc)\n",
    "    c,a,b,k,l=vae.test(doc)\n",
    "    cost.append(c)\n",
    "#     print c,n_d\n",
    "    e1.append(a)\n",
    "    e2.append(b)\n",
    "    e3.append(k)\n",
    "    e4.append(l)\n",
    "    if idx%1000==0:\n",
    "        print idx\n",
    "    \n",
    "print ((np.mean(np.array(cost))))\n",
    "# print np.mean(e1,0)\n",
    "# print np.mean(e2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1]\n",
      "[1 2]\n",
      "[ 2 36 49 13 22  7 37 14 24 46 32 44 16 26 48 18 23 25  9  1 50  8 40 20 12\n",
      "  6 31 10 28 11 15 27 29 34 42 38 33 21 41 45 39  3  5  4 30 19 47 17 43 35]\n",
      "[38 13 14 20 49 43  6 44 34 50 18 25 32 36 22  3 24  7 40 23 46 12  8 10 16\n",
      " 27  9 28  1 26 48 19  4 45  5 42 11  2 31 35 29 47 41 17 21 30 37 15 33 39]\n",
      "[[  1.00000000e+00   1.04386508e-07]]\n",
      "[[  1.46643613e-22   1.00000000e+00]]\n",
      "[[ 0.00334829  0.00536268  0.0056372   0.00964715  0.01178652  0.01187422\n",
      "   0.01188467  0.01232468  0.01255479  0.01256892  0.01261481  0.01279969\n",
      "   0.01294623  0.01468423  0.01486358  0.01507311  0.01508066  0.01690379\n",
      "   0.01752479  0.01775655  0.01783933  0.01795567  0.01801715  0.01852449\n",
      "   0.01868502  0.01873625  0.01886442  0.01917117  0.01997501  0.02026113\n",
      "   0.02043691  0.02197537  0.02254776  0.0235034   0.02368179  0.02381792\n",
      "   0.02418585  0.02497809  0.02586782  0.02708452  0.02931921  0.02949175\n",
      "   0.02979021  0.02984348  0.03071339  0.03142248  0.03168741  0.03430611\n",
      "   0.03759667  0.04247655]]\n",
      "[[ 0.00236781  0.00854771  0.00907938  0.00969462  0.01011567  0.01104336\n",
      "   0.01214972  0.01312446  0.01372858  0.01376753  0.01379884  0.01464016\n",
      "   0.01483911  0.01488012  0.01495944  0.01504154  0.01572119  0.01576121\n",
      "   0.01589682  0.01686745  0.01701072  0.01706066  0.01835809  0.0183948\n",
      "   0.0184803   0.01913693  0.01927023  0.01994797  0.02045287  0.020588\n",
      "   0.02102665  0.02174165  0.02197972  0.02347607  0.02494595  0.0250178\n",
      "   0.02612157  0.02612722  0.02618776  0.0265233   0.02664703  0.02813151\n",
      "   0.0284611   0.02988961  0.02995992  0.03030461  0.03140809  0.0319271\n",
      "   0.03444052  0.04095035]]\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "print np.argsort(np.mean(e1,0)[0])+1\n",
    "print np.argsort(np.mean(e2,0)[0])+1\n",
    "print np.argsort(np.mean(e3,0)[0])+1\n",
    "print np.argsort(np.mean(e4,0)[0])+1\n",
    "print np.mean(e1,0)#[0]\n",
    "print np.mean(e2,0)#[0]\n",
    "print np.sort(np.mean(e3,0))#[0]\n",
    "print np.sort(np.mean(e4,0))#[0]\n",
    "print np.exp(np.mean(np.array(cost)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(cost,open('cost','w'))\n",
    "pickle.dump(e1,open('e1','w'))\n",
    "pickle.dump(e2,open('e2','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.066614\n",
      "0.00266949\n"
     ]
    }
   ],
   "source": [
    "e=e2\n",
    "print np.amax(np.mean(e,0))\n",
    "print np.min(np.mean(e,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "37\n",
      "Epoch: 0001 cost= 1951.299849720\n",
      "avg_cost =  681.862591718\n",
      "37\n",
      "Epoch: 0002 cost= 1802.893371419\n",
      "avg_cost =  674.743327373\n",
      "37\n",
      "Epoch: 0003 cost= 2080.210038171\n",
      "avg_cost =  687.620173274\n",
      "37\n",
      "Epoch: 0004 cost= 2131.480287028\n",
      "avg_cost =  689.811478898\n",
      "37\n",
      "Epoch: 0005 cost= 1697.812007509\n",
      "avg_cost =  669.3386082\n",
      "37\n",
      "Epoch: 0006 cost= 1812.600706940\n",
      "avg_cost =  675.226615287\n",
      "37\n",
      "Epoch: 0007 cost= 2147.505452401\n",
      "avg_cost =  690.485597353\n",
      "37\n",
      "Epoch: 0008 cost= 1754.365650309\n",
      "avg_cost =  672.287635597\n",
      "37\n",
      "Epoch: 0009 cost= 1986.632824159\n",
      "avg_cost =  683.477679278\n",
      "37\n",
      "Epoch: 0010 cost= 1301.936128341\n",
      "avg_cost =  645.444698849\n"
     ]
    }
   ],
   "source": [
    "test_batch = create_minibatch(docs_te.astype('float32'))\n",
    "def test(vae,minibatches):\n",
    "    emb=0\n",
    "    avg_kld=0\n",
    "    print batch_size\n",
    "    # Training cycle\n",
    "    for epoch in range(10):\n",
    "        avg_cost = 0.\n",
    "        avg_kld = 0.\n",
    "        total_batch = int(n_samples_te / batch_size)\n",
    "        print total_batch\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs = minibatches.next()\n",
    "            # Fit training using batch data\n",
    "            cost,a,b = vae.batchtest(batch_xs)\n",
    "            cost=cost\n",
    "            # Compute average loss\n",
    "            avg_cost += cost / total_batch#n_samples_tr * batch_size\n",
    "\n",
    "\n",
    "        # Display logs per epoch step\n",
    "#         if epoch % display_step == 0:\n",
    "        cost_plot.append(avg_cost)\n",
    "        print \"Epoch:\", '%04d' % (epoch+1), \\\n",
    "              \"cost=\", \"{:.9f}\".format(np.exp((avg_cost/90)))\n",
    "        print 'avg_cost = ',avg_cost\n",
    "\n",
    "test(vae,test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.0701\n"
     ]
    }
   ],
   "source": [
    "print np.mean(np.sum(docs_tr.astype('float32'),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[2 1 0 7 3 8 4 5 6 9]\n",
    "thanks hus advance appreciate modem\n",
    "article write like get one\n",
    "god people bible faith one\n",
    "team rangers game player leafs\n",
    "people population arab village government\n",
    "god people christian truth evidence\n",
    "thanks motherboard sale hd ide\n",
    "virtual software interface graphic application\n",
    "government arab israeli turks population\n",
    "spacecraft encryption key km flight\n",
    "\n",
    "[4 1 9 2 8 6 5 0 3 7]\n",
    "thanks motherboard sale hd ide\n",
    "god people christian truth evidence\n",
    "virtual software interface graphic application\n",
    "god people bible faith one\n",
    "article write like get one\n",
    "people population arab village government\n",
    "spacecraft encryption key km flight\n",
    "thanks hus advance appreciate modem\n",
    "government arab israeli turks population\n",
    "team rangers game player leafs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
