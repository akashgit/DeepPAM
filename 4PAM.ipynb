{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/group/cup/data1/akash_itm/IPC_MAP/lib/python2.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools,time\n",
    "import sys, os\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset_tr = '/afs/inf.ed.ac.uk/group/cup/data1/akash_itm\\\n",
    "/IPC_MAP/NTP/tensorflow_lda/lda-tf_new/data/20news_clean/train.txt.npy'\n",
    "data_tr = np.load(dataset_tr)\n",
    "dataset_te = '/afs/inf.ed.ac.uk/group/cup/data1/akash_itm\\\n",
    "/IPC_MAP/NTP/tensorflow_lda/lda-tf_new/data/20news_clean/test.txt.npy'\n",
    "data_te = np.load(dataset_te)\n",
    "vocab = '/afs/inf.ed.ac.uk/group/cup/data1/akash_itm\\\n",
    "/IPC_MAP/NTP/tensorflow_lda/lda-tf_new/data/20news_clean/vocab.pkl'\n",
    "vocab = pickle.load(open(vocab,'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11258, 1995)\n",
      "(7487, 1995)\n"
     ]
    }
   ],
   "source": [
    "vocab_size=len(vocab)\n",
    "def onehot(data, min_length):\n",
    "    return np.bincount(data, minlength=min_length)\n",
    "data_tr = np.array([onehot(doc.astype('int'),vocab_size) for doc in data_tr if np.sum(doc)!=0])\n",
    "data_te = np.array([onehot(doc.astype('int'),vocab_size) for doc in data_te if np.sum(doc)!=0])\n",
    "print data_tr.shape\n",
    "print data_te.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_samples_tr = 11258\n",
    "n_samples_te = 7488\n",
    "docs_tr = data_tr\n",
    "docs_te = data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def xavier_init(fan_in, fan_out, constant=1): \n",
    "    \"\"\" Xavier initialization of network weights\"\"\"\n",
    "    # https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "    low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "    high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out), \n",
    "                             minval=low, maxval=high, \n",
    "                             dtype=tf.float32)\n",
    "def log_dir_init(fan_in, fan_out,topics=50): \n",
    "    return tf.log((1.0/topics)*tf.ones([fan_in, fan_out]))\n",
    "\n",
    "tf.reset_default_graph()\n",
    "class VariationalAutoencoder(object):\n",
    "    \"\"\" \n",
    "    See \"Auto-Encoding Variational Bayes\" by Kingma and Welling for more details.\n",
    "    \"\"\"\n",
    "    def __init__(self, network_architecture, transfer_fct=tf.nn.softplus, \n",
    "                 learning_rate=0.001, batch_size=100):\n",
    "        self.network_architecture = network_architecture\n",
    "        self.transfer_fct = transfer_fct\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        print 'Learning Rate:', self.learning_rate\n",
    "        \n",
    "        # tf Graph input\n",
    "        self.x = tf.placeholder(tf.float32, [None, network_architecture[\"n_input\"]])\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        self.is_training = tf.placeholder(tf.bool, [], name='is_training')\n",
    "        self.samples = tf.placeholder(tf.int32)\n",
    "        \n",
    "        self.h_dim = float(network_architecture[\"n_z\"])\n",
    "        self.a0 = np.ones((1 , 2)).astype(np.float32)/2\n",
    "        self.a1 = np.ones((1 , self.h_dim)).astype(np.float32)/self.h_dim\n",
    "        self.a2 = np.ones((1 , self.h_dim)).astype(np.float32)/self.h_dim\n",
    "        \n",
    "        self.mu2a0 = tf.constant((np.log(self.a0).T-np.mean(np.log(self.a0),1)).T)\n",
    "        self.var2a0 = tf.constant(  ( ( (1.0/self.a0)*( 1 - (2.0/2.0) ) ).T +\n",
    "                                ( 1.0/(self.h_dim*self.h_dim) )*np.sum(1.0/self.a0,1) ).T  )\n",
    "        \n",
    "        self.mu2a1 = tf.constant((np.log(self.a1).T-np.mean(np.log(self.a1),1)).T)\n",
    "        self.var2a1 = tf.constant(  ( ( (1.0/self.a1)*( 1 - (2.0/self.h_dim) ) ).T +\n",
    "                                ( 1.0/(self.h_dim*self.h_dim) )*np.sum(1.0/self.a1,1) ).T  )\n",
    "        \n",
    "        self.mu2a2 = tf.constant((np.log(self.a2).T-np.mean(np.log(self.a2),1)).T)\n",
    "        self.var2a2 = tf.constant(  ( ( (1.0/self.a2)*( 1 - (2.0/self.h_dim) ) ).T +\n",
    "                                ( 1.0/(self.h_dim*self.h_dim) )*np.sum(1.0/self.a2,1) ).T  )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Create autoencoder network\n",
    "        self._create_network()\n",
    "        # Define loss function based variational upper-bound and \n",
    "        # corresponding optimizer\n",
    "        self._create_loss_optimizer()\n",
    "        \n",
    "        # Initializing the tensor flow variables\n",
    "        init = tf.initialize_all_variables()\n",
    "\n",
    "        # Launch the session\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(init)\n",
    "    \n",
    "    def _create_network(self):\n",
    "        n_z = self.network_architecture[\"n_z\"]\n",
    "        # Initialize autoencode network weights and biases\n",
    "        self.network_weights = self._initialize_weights(**self.network_architecture)\n",
    "        '''Adding 2 subtopics and 1 super topic.'''\n",
    "        self.z_mean0,self.z_log_sigma_sq0 = \\\n",
    "            self._recognition_network(self.network_weights[\"weights_recog\"], \n",
    "                                      self.network_weights[\"biases_recog\"])\n",
    "            \n",
    "        self.z_mean_,self.z_log_sigma_sq_ = \\\n",
    "            self._recognition_network(self.network_weights[\"sub_01\"], \n",
    "                                      self.network_weights[\"biases_sub_01\"])\n",
    "            \n",
    "        self.z_mean1=self.z_mean_[:,:n_z]\n",
    "        self.z_mean2=self.z_mean_[:,n_z:]\n",
    "        self.z_log_sigma_sq1=self.z_log_sigma_sq_[:,:n_z]\n",
    "        self.z_log_sigma_sq2=self.z_log_sigma_sq_[:,n_z:]\n",
    "        \n",
    "#         self.z_mean2,self.z_log_sigma_sq2 = \\\n",
    "#             self._recognition_network(self.network_weights[\"sub_01\"], \n",
    "#                                       self.network_weights[\"biases_sub_01\"])\n",
    "            \n",
    "#         self.z_mean3,self.z_log_sigma_sq3 = \\\n",
    "#             self._recognition_network(self.network_weights[\"sub_02\"], \n",
    "#                                       self.network_weights[\"biases_sub_02\"])\n",
    "        \n",
    "#         self.z_mean4,self.z_log_sigma_sq4 = \\\n",
    "#             self._recognition_network(self.network_weights[\"sub_02\"], \n",
    "#                                       self.network_weights[\"biases_sub_02\"])\n",
    "\n",
    "        # Draw one sample z from Gaussian distribution\n",
    "        '''Adding another noise variable for super toipc: hard coding atm'''\n",
    "        eps0 = tf.random_normal((self.samples, 2), 0, 1, \n",
    "                               dtype=tf.float32)\n",
    "        \n",
    "        eps1 = tf.random_normal((self.samples, n_z), 0, 1, \n",
    "                               dtype=tf.float32)\n",
    "        eps2 = tf.random_normal((self.samples, n_z), 0, 1, \n",
    "                               dtype=tf.float32)\n",
    "        '''Adding RT for subtopics'''\n",
    "        self.z0 = tf.nn.softmax(tf.contrib.layers.batch_norm(tf.add(self.z_mean0, \n",
    "                        tf.mul(tf.sqrt(tf.exp(self.z_log_sigma_sq0)), eps0))))\n",
    "        self.z0 = tf.nn.softmax(tf.contrib.layers.batch_norm(self.z0,\n",
    "                                                             is_training=self.is_training,updates_collections=None))\n",
    "        \n",
    "        self.sigma0 = tf.exp(self.z_log_sigma_sq0)\n",
    "        \n",
    "        self.z1 = tf.add(self.z_mean1, \n",
    "                        tf.mul(tf.sqrt(tf.exp(self.z_log_sigma_sq1)), eps1))\n",
    "        self.z1 = tf.nn.softmax(tf.contrib.layers.batch_norm(self.z1,\n",
    "                                                            is_training=self.is_training,updates_collections=None))\n",
    "        \n",
    "        self.sigma1 = tf.exp(self.z_log_sigma_sq1)\n",
    "        \n",
    "        self.z2 = tf.add(self.z_mean2, \n",
    "                        tf.mul(tf.sqrt(tf.exp(self.z_log_sigma_sq2)), eps2))\n",
    "        self.z2 = tf.nn.softmax(tf.contrib.layers.batch_norm(self.z2,\n",
    "                                                            is_training=self.is_training,updates_collections=None))\n",
    "        \n",
    "        self.sigma2 = tf.exp(self.z_log_sigma_sq2)\n",
    "        \n",
    "#         self.z3 = tf.add(self.z_mean3, \n",
    "#                         tf.mul(tf.sqrt(tf.exp(self.z_log_sigma_sq3)), eps1))\n",
    "#         self.z3 = tf.nn.softmax(tf.contrib.layers.batch_norm(self.z3,\n",
    "#                                                             is_training=self.is_training,updates_collections=None))\n",
    "        \n",
    "#         self.sigma3 = tf.exp(self.z_log_sigma_sq3)\n",
    "        \n",
    "#         self.z4 = tf.add(self.z_mean4, \n",
    "#                         tf.mul(tf.sqrt(tf.exp(self.z_log_sigma_sq4)), eps2))\n",
    "#         self.z4 = tf.nn.softmax(tf.contrib.layers.batch_norm(self.z4,\n",
    "#                                                             is_training=self.is_training,updates_collections=None))\n",
    "        \n",
    "#         self.sigma4 = tf.exp(self.z_log_sigma_sq4)\n",
    "        \n",
    "        '''Adding subtopic reconstruction'''\n",
    "            \n",
    "#         self.theta = tf.matmul(self.z0,tf.concat(0,[self.z1, self.z2]))\n",
    "        \n",
    "        self.theta = tf.squeeze(tf.map_fn(lambda i: \n",
    "                                          tf.matmul(tf.expand_dims(tf.gather(self.z0,i),0),\n",
    "                                                    tf.concat(0,[tf.expand_dims(tf.gather(self.z1,i),0),\n",
    "                                                                tf.expand_dims(tf.gather(self.z2,i),0)])),\n",
    "                                                   tf.constant(np.arange(self.batch_size)), dtype=tf.float32))\n",
    "            \n",
    "        self.x_reconstr_mean = \\\n",
    "            self._generator_network(tf.nn.dropout(self.theta, self.keep_prob),\n",
    "                                    self.network_weights[\"weights_gener\"],\n",
    "                                    self.network_weights[\"biases_gener\"])\n",
    "            \n",
    "    def _initialize_weights(self, n_hidden_recog_1, n_hidden_recog_2, \n",
    "                            n_hidden_gener_1,  \n",
    "                            n_input, n_z):\n",
    "        all_weights = dict()\n",
    "        with tf.variable_scope(\"super\"):\n",
    "            all_weights['weights_recog'] = {\n",
    "                'h1': tf.get_variable('h1',[n_input, n_hidden_recog_1]),\n",
    "                'h2': tf.get_variable('h2',[n_hidden_recog_1, n_hidden_recog_2]),\n",
    "                'out_mean': tf.get_variable('out_mean',[n_hidden_recog_2, 2]),\n",
    "                'out_log_sigma': tf.get_variable('out_log_sigma',[n_hidden_recog_2, 2])\n",
    "            }\n",
    "            all_weights['biases_recog'] = {\n",
    "                'b1': tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32)),\n",
    "                'b2': tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32)),\n",
    "                'out_mean': tf.Variable(tf.zeros([2], dtype=tf.float32)),\n",
    "                'out_log_sigma': tf.Variable(tf.zeros([2], dtype=tf.float32))\n",
    "            }\n",
    "        with tf.variable_scope(\"sub01\"):\n",
    "            all_weights['sub_01'] = {\n",
    "                'h1': tf.get_variable('h1',[n_input, n_hidden_recog_1]),\n",
    "                'h2': tf.get_variable('h2',[n_hidden_recog_1, n_hidden_recog_2]),\n",
    "                'out_mean': tf.get_variable('out_mean',[n_hidden_recog_2, n_z*2]),\n",
    "                'out_log_sigma': tf.get_variable('out_log_sigma',[n_hidden_recog_2, n_z*2])}\n",
    "            all_weights['biases_sub_01'] = {\n",
    "                'b1': tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32)),\n",
    "                'b2': tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32)),\n",
    "                'out_mean': tf.Variable(tf.zeros([n_z*2], dtype=tf.float32)),\n",
    "                'out_log_sigma': tf.Variable(tf.zeros([n_z*2], dtype=tf.float32))\n",
    "            }\n",
    "        with tf.variable_scope(\"sub02\"):\n",
    "            all_weights['sub_02'] = {\n",
    "                'h1': tf.get_variable('h1',[n_input, n_hidden_recog_1]),\n",
    "                'h2': tf.get_variable('h2',[n_hidden_recog_1, n_hidden_recog_2]),\n",
    "                'out_mean': tf.get_variable('out_mean',[n_hidden_recog_2, n_z]),\n",
    "                'out_log_sigma': tf.get_variable('out_log_sigma',[n_hidden_recog_2, n_z])\n",
    "            }\n",
    "            all_weights['biases_sub_02'] = {\n",
    "                'b1': tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32)),\n",
    "                'b2': tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32)),\n",
    "                'out_mean': tf.Variable(tf.zeros([n_z], dtype=tf.float32)),\n",
    "                'out_log_sigma': tf.Variable(tf.zeros([n_z], dtype=tf.float32))\n",
    "            }\n",
    "        with tf.variable_scope(\"gen\"):\n",
    "            all_weights['weights_gener'] = {\n",
    "                'h2': tf.Variable(xavier_init(n_z, n_hidden_gener_1))\n",
    "            }\n",
    "            all_weights['biases_gener'] = {\n",
    "                'b2': tf.Variable(tf.zeros([n_hidden_gener_1], dtype=tf.float32))\n",
    "            }\n",
    "        return all_weights\n",
    "            \n",
    "    def _recognition_network(self, weights, biases):\n",
    "        # Generate probabilistic encoder (recognition network)\n",
    "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.x, weights['h1']), \n",
    "                                           biases['b1'])) \n",
    "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
    "                                           biases['b2'])) \n",
    "        layer_do = tf.nn.dropout(layer_2, self.keep_prob)\n",
    "        \n",
    "        z_mean = tf.contrib.layers.batch_norm(tf.add(tf.matmul(layer_do, weights['out_mean']),\n",
    "                        biases['out_mean']),is_training=self.is_training,updates_collections=None)\n",
    "        z_log_sigma_sq = \\\n",
    "            tf.contrib.layers.batch_norm(tf.add(tf.matmul(layer_do, weights['out_log_sigma']), \n",
    "                   biases['out_log_sigma']),is_training=self.is_training,updates_collections=None)     \n",
    "        \n",
    "        return (z_mean, z_log_sigma_sq)\n",
    "\n",
    "    def _generator_network(self,z, weights, biases):\n",
    "        # Generate probabilistic decoder (decoder network)\n",
    "#         self.layer_do_0 = tf.nn.dropout(tf.nn.softmax(tf.contrib.layers.batch_norm(z)), self.keep_prob)\n",
    "        self.layer_do_0 =z\n",
    "        x_reconstr_mean = tf.add(tf.matmul(self.layer_do_0, \n",
    "                                           tf.nn.softmax(tf.contrib.layers.batch_norm(weights['h2'],\\\n",
    "is_training=self.is_training,updates_collections=None))),0.0)\n",
    "#         x_reconstr_mean = tf.nn.softmax(tf.contrib.layers.batch_norm(tf.add(\n",
    "#                     tf.matmul(self.layer_do_0, weights['h2']),0.0)))\n",
    "    \n",
    "        return x_reconstr_mean\n",
    "\n",
    "            \n",
    "    def _create_loss_optimizer(self):\n",
    "        \n",
    "        self.x_reconstr_mean+=1e-10\n",
    "     \n",
    "        reconstr_loss = \\\n",
    "            -tf.reduce_sum(self.x * tf.log(self.x_reconstr_mean),1)#/tf.reduce_sum(self.x,1) \n",
    "\n",
    "    \n",
    "        latent_loss0 = 0.5*( tf.reduce_sum(tf.div(self.sigma0,self.var2a0),1)+\\\n",
    "        tf.reduce_sum( tf.mul(tf.div((self.mu2a0 - self.z_mean0),self.var2a0),\n",
    "                  (self.mu2a0 - self.z_mean0)),1) - 2 +\\\n",
    "                           tf.reduce_sum(tf.log(self.var2a0),1)  - tf.reduce_sum(self.z_log_sigma_sq0  ,1) ) \n",
    "        \n",
    "        latent_loss1 = 0.5*( tf.reduce_sum(tf.div(self.sigma1,self.var2a1),1)+\\\n",
    "        tf.reduce_sum( tf.mul(tf.div((self.mu2a1 - self.z_mean1),self.var2a1),\n",
    "                  (self.mu2a1 - self.z_mean1)),1) - self.h_dim +\\\n",
    "                           tf.reduce_sum(tf.log(self.var2a1),1)  - tf.reduce_sum(self.z_log_sigma_sq1  ,1) )\n",
    "        \n",
    "        latent_loss2 = 0.5*( tf.reduce_sum(tf.div(self.sigma2,self.var2a2),1)+\\\n",
    "        tf.reduce_sum( tf.mul(tf.div((self.mu2a2 - self.z_mean2),self.var2a2),\n",
    "                  (self.mu2a2 - self.z_mean2)),1) - self.h_dim +\\\n",
    "                           tf.reduce_sum(tf.log(self.var2a2),1)  - tf.reduce_sum(self.z_log_sigma_sq2  ,1) )\n",
    "\n",
    "        \n",
    "        \n",
    "#         latent_loss = -0.5 * tf.reduce_sum(1 + self.z_log_sigma_sq \n",
    "#                                            - tf.square(self.z_mean) \n",
    "#                                            - tf.exp(self.z_log_sigma_sq), 1)\n",
    "\n",
    "\n",
    "        self.cost = tf.reduce_mean(reconstr_loss) \\\n",
    "    + tf.reduce_mean(latent_loss0 + latent_loss1 + latent_loss2) # average over batch\n",
    "        \n",
    "        # Use ADAM optimizer\n",
    "        self.optimizer = \\\n",
    "            tf.train.AdamOptimizer(learning_rate=self.learning_rate,beta1=0.9).minimize(self.cost)\n",
    "        \n",
    "    def partial_fit(self, X):\n",
    "        \"\"\"Train model based on mini-batch of input data.\n",
    "        \n",
    "        Return cost of mini-batch.\n",
    "        \"\"\"\n",
    "#         opt, cost,emb,kld = self.sess.run((self.optimizer, self.cost,self.network_weights['weights_gener']['h2'],\n",
    "#                                           self.kld),feed_dict={self.x: X,self.keep_prob: .9})\n",
    "        opt,cost,emb = self.sess.run((self.optimizer, self.cost,self.network_weights['weights_gener']['h2']\n",
    "                                      ),feed_dict={self.x: X,self.keep_prob: .7,self.is_training:True,self.samples:1})\n",
    "        return cost,emb\n",
    "    \n",
    "    def test(self, X):\n",
    "        \"\"\"Test the model and return the lowerbound on the log-likelihood.\n",
    "        \"\"\"\n",
    "        cost,e1,e2 = self.sess.run((self.cost,self.z1,self.z2),\n",
    "                                        feed_dict={self.x: np.expand_dims(X, axis=0),self.keep_prob: 1.0,\n",
    "                                                   self.is_training:False,self.samples:1})\n",
    "        return cost,e1,e2\n",
    "    \n",
    "    def batchtest(self, X):\n",
    "        \"\"\"Test the model and return the lowerbound on the log-likelihood.\n",
    "        \"\"\"\n",
    "        cost,e1,e2 = self.sess.run((self.cost,self.z1,self.z2),\n",
    "                                        feed_dict={self.x: X,self.keep_prob: 1.0,\n",
    "                                                   self.is_training:False,self.samples:1})\n",
    "        return cost,e1,e2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "cost_plot=[]\n",
    "test_batch = create_minibatch(docs_tr.astype('float32'))\n",
    "def train(network_architecture, learning_rate=0.0001,\n",
    "          batch_size=100, training_epochs=10, display_step=5):\n",
    "    vae = VariationalAutoencoder(network_architecture, \n",
    "                                 learning_rate=learning_rate, \n",
    "                                 batch_size=batch_size)\n",
    "    emb=0\n",
    "    avg_kld=0\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        avg_kld=0.\n",
    "        total_batch = int(n_samples_tr / batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs = minibatches.next()\n",
    "            # Fit training using batch data\n",
    "            cost,emb = vae.partial_fit(batch_xs)\n",
    "            c,a,b = vae.batchtest(batch_xs)\n",
    "\n",
    "#             sys.exit()\n",
    "            # Compute average loss\n",
    "            avg_cost += cost / n_samples_tr * batch_size\n",
    "            avg_kld += c / n_samples_tr * batch_size\n",
    "            \n",
    "            if np.isnan(avg_cost):\n",
    "                print epoch,i,np.sum(batch_xs,1).astype(np.int),batch_xs.shape\n",
    "                return vae,emb\n",
    "#                 sys.exit()\n",
    "        \n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            cost_plot.append(avg_cost)\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \\\n",
    "                  \"cost=\", \"{:.9f}\".format(avg_cost)\n",
    "            print 'avg_cost = ',avg_kld\n",
    "            test_c=0.\n",
    "            for i in range(int(n_samples_te / batch_size)):\n",
    "                c,a,b = vae.batchtest(test_batch.next())\n",
    "                test_c+=c / n_samples_te * batch_size\n",
    "            print 'avg_test_cost = ',test_c\n",
    "    return vae,emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/group/cup/data1/akash_itm/IPC_MAP/lib/python2.7/site-packages/numpy/core/numeric.py:190: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  a = empty(shape, dtype, order)\n",
      "/afs/inf.ed.ac.uk/group/cup/data1/akash_itm/IPC_MAP/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py:90: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.001\n",
      "Epoch: 0001 cost= 2421.477808489\n",
      "avg_cost =  2834.97994308\n",
      "avg_test_cost =  2923.33809616\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-bb649bb24d11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mminibatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs_tr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mvae\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0memb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork_architecture\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;31m# plt.plot(cost_plot)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'it took'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'seconds'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-cbb26f8134ac>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(network_architecture, learning_rate, batch_size, training_epochs, display_step)\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mbatch_xs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminibatches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m# Fit training using batch data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0memb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatchtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-4c153cd33481>\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;31m#                                           self.kld),feed_dict={self.x: X,self.keep_prob: .9})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         opt,cost,emb = self.sess.run((self.optimizer, self.cost,self.network_weights['weights_gener']['h2']\n\u001b[1;32m--> 268\u001b[1;33m                                       ),feed_dict={self.x: X,self.keep_prob: .7,self.is_training:True,self.samples:1})\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/afs/inf.ed.ac.uk/group/cup/data1/akash_itm/IPC_MAP/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    715\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 717\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    718\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/afs/inf.ed.ac.uk/group/cup/data1/akash_itm/IPC_MAP/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    913\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 915\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    916\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/afs/inf.ed.ac.uk/group/cup/data1/akash_itm/IPC_MAP/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 965\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/afs/inf.ed.ac.uk/group/cup/data1/akash_itm/IPC_MAP/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    970\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    971\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 972\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    973\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/afs/inf.ed.ac.uk/group/cup/data1/akash_itm/IPC_MAP/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m    955\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "tf.reset_default_graph()\n",
    "network_architecture = \\\n",
    "    dict(n_hidden_recog_1=100, # 1st layer encoder neurons\n",
    "         n_hidden_recog_2=100, # 2nd layer encoder neurons\n",
    "         n_hidden_gener_1=1995, # 1st layer decoder neurons\n",
    "         n_input=1995, # MNIST data input (img shape: 28*28)\n",
    "         n_z=50)  # dimensionality of latent space\n",
    "\n",
    "batch_size=200\n",
    "learning_rate=0.001\n",
    "def create_minibatch(data):\n",
    "    rng = np.random.RandomState(10)\n",
    "    \n",
    "    while True:\n",
    "        # Return random data samples of a size 'minibatch_size' at each iteration\n",
    "        ixs = rng.randint(data.shape[0], size=batch_size)\n",
    "        yield data[ixs]\n",
    "        \n",
    "minibatches = create_minibatch(docs_tr.astype('float32'))\n",
    "start = time.time()\n",
    "vae,emb = train(network_architecture, training_epochs=200,batch_size=batch_size,learning_rate=learning_rate)\n",
    "# plt.plot(cost_plot)\n",
    "print 'it took',str(time.time()-start),'seconds'\n",
    "\n",
    "\n",
    "'''\n",
    "Change log: decoder has no dropout or additional tranformations\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flight space shuttle orbit solar\n",
      "sale shipping price speaker interested\n",
      "people massacre armenia armenians armenian\n",
      "game team player playoff score\n",
      "like article write get think\n",
      "june annual app ed cover\n",
      "card monitor modem video problem\n",
      "pixel server display version color\n",
      "drive ide hd scsi connector\n",
      "condition worth sale nice experience\n",
      "game team player playoff rangers\n",
      "anyone anybody please phone thank\n",
      "public issue control social paper\n",
      "compression smaller faq requirement distribution\n",
      "space laboratory nasa university research\n",
      "electronics sell audio signal frequency\n",
      "big pat spring pretty lot\n",
      "car bike engine ride buy\n",
      "hus thanks appreciate anyone hello\n",
      "card problem monitor motherboard printer\n",
      "article write people like make\n",
      "bike ride rear front helmet\n",
      "motherboard card scsi meg hd\n",
      "car heat expensive cheaper ford\n",
      "hus thanks appreciate hello advance\n",
      "mw mg mi sl oo\n",
      "adams gary san blue mike\n",
      "god christian faith believe jesus\n",
      "spacecraft flight orbit km space\n",
      "key chip government encryption clipper\n",
      "shoot tank cop gang fire\n",
      "fax distribute available ftp systems\n",
      "car tube voltage honda battery\n",
      "article write like get time\n",
      "character size program hint variable\n",
      "program file copyright section info\n",
      "university space research flight institute\n",
      "god believe evidence christian sexual\n",
      "go work people say know\n",
      "god christian islamic believe religion\n",
      "god christian bible faith believe\n",
      "game player baseball team fan\n",
      "billion cost estimate year budget\n",
      "surrender waco gordon msg batf\n",
      "windows software directory postscript window\n",
      "israel israeli arab arabs innocent\n",
      "israel israeli government arabs arab\n",
      "key chip algorithm clipper escrow\n",
      "window button graphic app screen\n",
      "clinton government federal economic bush\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(beta, feature_names, n_top_words=5):\n",
    "    for i in range(len(beta)):\n",
    "        print(\" \".join([feature_names[j]\n",
    "            for j in beta[i].argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "print_top_words(emb, zip(*sorted(vocab.items(), key=lambda x: x[1]))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "2702.9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cost=[]\n",
    "e1=[]\n",
    "e2=[]\n",
    "idx=0\n",
    "for doc in docs_te:\n",
    "    idx+=1\n",
    "    doc = doc.astype('float32')\n",
    "    n_d = np.sum(doc)\n",
    "    c,a,b=vae.test(doc)\n",
    "    cost.append(c)\n",
    "#     print c,n_d\n",
    "    e1.append(a)\n",
    "    e2.append(b)\n",
    "    if idx%1000==0:\n",
    "        print idx\n",
    "    \n",
    "print ((np.mean(np.array(cost))))\n",
    "# print np.mean(e1,0)\n",
    "# print np.mean(e2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9 25 15 28 16  7  8 13 11  5 31 26 38 22 12  0 34 47 42  1 33 14 30  2 35\n",
      " 48 24 23 29 41 21 27 43  3 32 49 37 45  4 40 10 18 46 39 36 44 17 20 19  6]\n",
      "[ 9 13  5 25 38  0  8 15 31 28 34 16 35 49 12 14 30 47  2 21 26 42 29  1 36\n",
      " 11 23 46 45 22  7 40 39 10 24 48 43  3 44 32 41 18 20 27 19 17 37  4  6 33]\n",
      "[[ 0.01680556  0.01851116  0.01903432  0.02198341  0.02280767  0.0156597\n",
      "   0.0342755   0.01429252  0.01497298  0.01207014  0.02440056  0.01545859\n",
      "   0.01676555  0.01526901  0.01875586  0.01251809  0.01276207  0.02719749\n",
      "   0.02442865  0.03283479  0.03163897  0.02113     0.01642988  0.02040565\n",
      "   0.01980613  0.01246835  0.01609043  0.02125806  0.01260613  0.0206102\n",
      "   0.01876375  0.01565973  0.02230362  0.01861605  0.01747276  0.01913946\n",
      "   0.02503147  0.02257272  0.01632066  0.02476188  0.02283805  0.02067348\n",
      "   0.01843373  0.02132604  0.0258192   0.02272926  0.02474192  0.0177258\n",
      "   0.01934905  0.02246682]]\n",
      "[[ 0.01336075  0.01883288  0.01783204  0.02418263  0.02974392  0.01196492\n",
      "   0.02977409  0.02054426  0.01378759  0.00773435  0.02215422  0.01992038\n",
      "   0.01600035  0.01132737  0.01673797  0.01381737  0.0147118   0.02939078\n",
      "   0.02572369  0.02912083  0.02819966  0.01792265  0.02050921  0.01993225\n",
      "   0.02264102  0.01217461  0.0184428   0.02838413  0.01429334  0.01867801\n",
      "   0.01710919  0.01415245  0.0253514   0.03442275  0.01441316  0.01495604\n",
      "   0.01916685  0.0294926   0.01220309  0.02131381  0.02092726  0.02536966\n",
      "   0.01860209  0.0238452   0.02430638  0.02019975  0.01993645  0.01740313\n",
      "   0.02306484  0.01591667]]\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "print np.argsort(np.mean(e1,0)[0])\n",
    "print np.argsort(np.mean(e2,0)[0])\n",
    "print np.mean(e1,0)#[0]\n",
    "print np.mean(e2,0)#[0]\n",
    "print np.exp(np.mean(np.array(cost)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(cost,open('cost','w'))\n",
    "pickle.dump(e1,open('e1','w'))\n",
    "pickle.dump(e2,open('e2','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.066614\n",
      "0.00266949\n"
     ]
    }
   ],
   "source": [
    "e=e2\n",
    "print np.amax(np.mean(e,0))\n",
    "print np.min(np.mean(e,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "37\n",
      "Epoch: 0001 cost= 1951.299849720\n",
      "avg_cost =  681.862591718\n",
      "37\n",
      "Epoch: 0002 cost= 1802.893371419\n",
      "avg_cost =  674.743327373\n",
      "37\n",
      "Epoch: 0003 cost= 2080.210038171\n",
      "avg_cost =  687.620173274\n",
      "37\n",
      "Epoch: 0004 cost= 2131.480287028\n",
      "avg_cost =  689.811478898\n",
      "37\n",
      "Epoch: 0005 cost= 1697.812007509\n",
      "avg_cost =  669.3386082\n",
      "37\n",
      "Epoch: 0006 cost= 1812.600706940\n",
      "avg_cost =  675.226615287\n",
      "37\n",
      "Epoch: 0007 cost= 2147.505452401\n",
      "avg_cost =  690.485597353\n",
      "37\n",
      "Epoch: 0008 cost= 1754.365650309\n",
      "avg_cost =  672.287635597\n",
      "37\n",
      "Epoch: 0009 cost= 1986.632824159\n",
      "avg_cost =  683.477679278\n",
      "37\n",
      "Epoch: 0010 cost= 1301.936128341\n",
      "avg_cost =  645.444698849\n"
     ]
    }
   ],
   "source": [
    "test_batch = create_minibatch(docs_te.astype('float32'))\n",
    "def test(vae,minibatches):\n",
    "    emb=0\n",
    "    avg_kld=0\n",
    "    print batch_size\n",
    "    # Training cycle\n",
    "    for epoch in range(10):\n",
    "        avg_cost = 0.\n",
    "        avg_kld = 0.\n",
    "        total_batch = int(n_samples_te / batch_size)\n",
    "        print total_batch\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs = minibatches.next()\n",
    "            # Fit training using batch data\n",
    "            cost,a,b = vae.batchtest(batch_xs)\n",
    "            cost=cost\n",
    "            # Compute average loss\n",
    "            avg_cost += cost / total_batch#n_samples_tr * batch_size\n",
    "\n",
    "\n",
    "        # Display logs per epoch step\n",
    "#         if epoch % display_step == 0:\n",
    "        cost_plot.append(avg_cost)\n",
    "        print \"Epoch:\", '%04d' % (epoch+1), \\\n",
    "              \"cost=\", \"{:.9f}\".format(np.exp((avg_cost/90)))\n",
    "        print 'avg_cost = ',avg_cost\n",
    "\n",
    "test(vae,test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.0701\n"
     ]
    }
   ],
   "source": [
    "print np.mean(np.sum(docs_tr.astype('float32'),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
